{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"colab":{"name":"Copy of CSE527_HW3_Fall2019.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"name":"HW3.ipynb"},"cells":[{"cell_type":"code","metadata":{"id":"k4dhRR_119C4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614227203577,"user_tz":300,"elapsed":9632,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"127d673e-f6c1-456a-debd-bc83cb4d99bc"},"source":["pip install opencv-contrib-python==3.4.2.17"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting opencv-contrib-python==3.4.2.17\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/32/8d32d40cd35e61c80cb112ef5e8dbdcfbb06124f36a765df98517a12e753/opencv_contrib_python-3.4.2.17-cp37-cp37m-manylinux1_x86_64.whl (30.6MB)\n","\u001b[K     |████████████████████████████████| 30.6MB 153kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==3.4.2.17) (1.19.5)\n","Installing collected packages: opencv-contrib-python\n","  Found existing installation: opencv-contrib-python 4.1.2.30\n","    Uninstalling opencv-contrib-python-4.1.2.30:\n","      Successfully uninstalled opencv-contrib-python-4.1.2.30\n","Successfully installed opencv-contrib-python-3.4.2.17\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TxrQ1z4sObHr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614227208872,"user_tz":300,"elapsed":5290,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"50c4481f-b4af-4018-d80a-7bd4bef00b3b"},"source":["# import packages here\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import itertools\n","import time\n","import zipfile\n","import torch\n","import torchvision\n","import gc\n","import pickle\n","from sklearn import svm\n","from skimage import color\n","from skimage import io\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.cluster import KMeans\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import accuracy_score\n","\n","print(cv2.__version__) # verify OpenCV version"],"execution_count":3,"outputs":[{"output_type":"stream","text":["3.4.2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FhkgVLuQTHn7"},"source":["## Data Preparation"]},{"cell_type":"code","metadata":{"id":"ENRccp9L2X1F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614228599598,"user_tz":300,"elapsed":1351452,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"1746bfe5-f017-4e45-b201-5f4e8b4e5e4e"},"source":["class_names = [name[13:] for name in glob.glob('./data/train/*')]\n","class_names = dict(zip(range(len(class_names)), class_names))\n","print(\"class_names: %s \" % class_names)\n","n_train_samples_per_class = 150\n","n_test_samples_per_class = 50\n","\n","# To load images from the path provided\n","def load_dataset(path, num_per_class=-1):\n","    data = []\n","    labels = []\n","    for id, class_name in class_names.items():\n","        print(\"Loading images from class: %s\" % id)\n","        img_path_class = glob.glob(path + class_name + '/*.jpg')\n","        if num_per_class > 0:\n","            img_path_class = img_path_class[:num_per_class]\n","        labels.extend([id]*len(img_path_class))\n","        for filename in img_path_class:\n","            data.append(cv2.imread(filename, 0))\n","    return data, labels\n","\n","# load training dataset\n","# train_data, train_label = load_dataset('./data/train/')\n","train_data, train_label = load_dataset('./data/train/', n_train_samples_per_class)\n","n_train = len(train_label)\n","print(\"n_train: %s\" % n_train)\n","\n","# load testing dataset\n","# test_data, test_label = load_dataset('./data/test/')\n","test_data, test_label = load_dataset('./data/test/', n_test_samples_per_class)\n","n_test = len(test_label)\n","print(\"n_test: %s\" % n_test)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["class_names: {0: 'Forest', 1: 'Industrial', 2: 'Flower', 3: 'Coast', 4: 'InsideCity', 5: 'Office', 6: 'Bedroom', 7: 'Highway', 8: 'Street', 9: 'TallBuilding', 10: 'LivingRoom', 11: 'Suburb', 12: 'OpenCountry', 13: 'Mountain', 14: 'Kitchen', 15: 'Store'} \n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","n_train: 2400\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","n_test: 400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dwetz2PrcgWu","executionInfo":{"status":"ok","timestamp":1614228608990,"user_tz":300,"elapsed":5945,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}}},"source":["# As loading the data from the source for the first time is time consuming, so you can pkl or save the data in a compact way such that subsequent data loading is faster\r\n","# Save intermediate image data into disk\r\n","file = open('train.pkl','wb')\r\n","pickle.dump(train_data, file)\r\n","pickle.dump(train_label, file)\r\n","file.close()\r\n","\r\n","file = open('test.pkl','wb')\r\n","pickle.dump(test_data, file)\r\n","pickle.dump(test_label, file)\r\n","file.close()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"sS51rm2O24lm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614228613280,"user_tz":300,"elapsed":818,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"6b719496-380b-4b8e-fd51-a4a373180146"},"source":["# Load intermediate image data from disk\n","file = open('train.pkl', 'rb')\n","train_data = pickle.load(file)\n","train_label = pickle.load(file)\n","file.close()\n","\n","file = open('test.pkl', 'rb')\n","test_data = pickle.load(file)\n","test_label = pickle.load(file)\n","file.close()\n","\n","print(len(train_data), len(train_label)) # Verify number of training samples\n","print(len(test_data), len(test_label))   # Verify number of testing samples"],"execution_count":9,"outputs":[{"output_type":"stream","text":["2400 2400\n","400 400\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yMXRub-wboDK","executionInfo":{"status":"ok","timestamp":1614228624012,"user_tz":300,"elapsed":916,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}}},"source":["# plt.imshow(train_data[1], cmap='gray') # Verify image\n","img_new_size = (240, 240)\n","\n","# resizing the images\n","train_data = list(map(lambda x: cv2.resize(x, img_new_size), train_data))\n","train_data = np.stack(train_data)\n","train_label = np.array(train_label)\n","\n","test_data = list(map(lambda x: cv2.resize(x, img_new_size), test_data))\n","test_data = np.stack(test_data)\n","test_label = np.array(test_label)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FflGhZCeHLR"},"source":["# # Verify image\n","# plt.imshow(cv2.resize(train_data[1], img_new_size), cmap='gray')\n","# print(train_data[0].dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E39ihfKA3FtT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614228630129,"user_tz":300,"elapsed":455,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"78612915-ffed-43ef-863a-16448f1c44ba"},"source":["n_train = len(train_label)\n","n_test = len(test_label)\n","\n","# feature extraction\n","def extract_feat(raw_data):\n","    print(len(raw_data))\n","    feat_dim = 1000\n","    feat = np.zeros((len(raw_data), feat_dim), dtype=np.float32)\n","    for i in np.arange(feat.shape[0]):\n","        feat[i] = np.reshape(raw_data[i], (raw_data[i].size))[:feat_dim] # dummy implemtation\n","    print(\"feat\",len(feat))\n","    \n","    return feat\n","\n","train_feat = extract_feat(train_data)\n","test_feat = extract_feat(test_data)\n","\n","# model training: take feature and label, return model\n","def train(X, Y):\n","    return 0 # dummy implementation\n","\n","# prediction: take feature and model, return label\n","def predict(model, x):\n","    return np.random.randint(16) # dummy implementation\n","\n","# evaluation\n","predictions = [-1]*len(test_feat)\n","for i in np.arange(n_test):\n","    predictions[i] = predict(None, test_feat[i])\n","    \n","accuracy = sum(np.array(predictions) == test_label) / float(n_test)\n","\n","print(\"The accuracy of my dummy model is {:.2f}%\".format(accuracy*100))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2400\n","feat 2400\n","400\n","feat 400\n","The accuracy of my dummy model is 8.25%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"iPF5y8C4L4mC"},"source":["## Tiny Image Representation + Nearest Neighbor Classifier\n","\n","**resize the image to 16x16** and the tiny image is made to have zero mean and unit length (normalization)."]},{"cell_type":"code","metadata":{"id":"JBTiTro73Q5E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614228728035,"user_tz":300,"elapsed":1185,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"7b95c094-30a6-4fbd-8292-b87f24882757"},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import NearestNeighbors\n","\n","def resize_data(train_data, test_data):\n","    # for resizing the input images\n","    img_s = 16\n","    img_new_size = (img_s, img_s)\n","    # resizing the images\n","    train_X = list(map(lambda x: cv2.resize(x, img_new_size).flatten(), train_data))\n","    train_X = np.stack(train_X)\n","    test_X = list(map(lambda x: cv2.resize(x, img_new_size).flatten(), test_data))\n","    test_X = np.stack(test_X)\n","\n","    # for normalizing the images by moving the mean to zero and unit length\n","    train_X = train_X.astype(float)\n","    test_X = test_X.astype(float)\n","    for i in range(train_X.shape[0]):\n","        train_X[i] = (train_X[i]-train_X[i].mean())/train_X[i].std()\n","\n","    for i in range(test_X.shape[0]):\n","        test_X[i] = (test_X[i]-test_X[i].mean())/test_X[i].std()\n","\n","    return train_X, test_X\n","\n","def tiny_image_predict(train_data, train_label, test_data, test_label):\n","    train_X, test_X = resize_data(train_data, test_data)\n","    # K-nearest neighbors classifier model\n","    # model = KNeighborsClassifier(n_neighbors=27, algorithm='ball_tree', weights= 'distance')\n","    model = KNeighborsClassifier(n_neighbors=45)\n","\n","    # training the model\n","    model.fit(train_X, train_label)\n","    # prediction using the trained model\n","    predicted_labels = model.predict(test_X)\n","    return predicted_labels\n","  \n","pred1 = tiny_image_predict(train_data, train_label, test_data, test_label)\n","label1 = test_label\n","# accuracy of the model\n","prediction_accuracy = accuracy_score(pred1, test_label)\n","print(\"Accuracy: {:.2f}%\".format(prediction_accuracy * 100))\n","\n","# pred1, label1 = # train_and_test(..."],"execution_count":12,"outputs":[{"output_type":"stream","text":["Accuracy: 22.50%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"bU8G7eLyL4mH"},"source":["## Bag of SIFT Representation + Nearest Neighbor Classifer\n","A vocabulary of visual words is formed by sampling many local features from our training set and then cluster them with k-means. The number of k-means clusters is the size of our vocabulary and the size of our features. For example clustering many SIFT descriptors into k=50 clusters. For any new SIFT feature, we can figure out which region it belongs to with reference to the centroids of our original clusters. Those centroids are our visual word vocabulary.\n","\n","For each image SIFT descriptors are sampled and instead of storing hundreds of SIFT descriptors, we simply count how many SIFT descriptors fall into each cluster in our visual word vocabulary. This is done by finding the nearest neighbor k-means centroid for every SIFT feature. Thus, if we have a vocabulary of 50 visual words, and we detect 220 distinct SIFT features in an image, our bag of SIFT representation will be a histogram of 50 dimensions where each bin counts how many times a SIFT descriptor was assigned to that cluster. The total of all the bin-counts is 220. The histogram should be normalized so that image size does not dramatically change the bag of features magnitude.\n","\n","Using the Bag of SIFT feature representation of the images a KNN classifier is trained."]},{"cell_type":"code","metadata":{"id":"vu-tYYGz30L2"},"source":["import gc\n","import cv2\n","from sklearn.cluster import KMeans\n","import time\n","\n","# To generate keypoints of an image\n","def generate_keypoints(ref_img, step_size = 10):\n","    # keypoint generation from step_size\n","    keypoints = [cv2.KeyPoint(x_cord, y_cord, step_size) for y_cord in range(0, ref_img.shape[0], step_size) \n","                                               for x_cord in range(0, ref_img.shape[1], step_size)]\n","    return keypoints\n","\n","#To extract sift descriptors from the given images using key points  \n","def extract_sift_descriptors(train_data, test_data, key_points):\n","    train_descriptor_list = []\n","    test_descriptor_list = []\n","    # initializing sift extractor\n","    sift_feature_extractor = cv2.xfeatures2d.SIFT_create()\n","\n","    # extracting descriptors for the selected keypoints from train and test data\n","    for image in train_data:\n","      kp, descriptors = sift_feature_extractor.compute(image, keypoints)\n","      train_descriptor_list.append(descriptors)\n","\n","    for image in test_data:\n","      kp, descriptors = sift_feature_extractor.compute(image, keypoints)\n","      test_descriptor_list.append(descriptors)\n","\n","    # garbage collection just in case\n","    gc.collect()\n","      \n","    # modifying the shape  \n","    train_descriptor_array = np.array(train_descriptor_list[0])\n","    for des in train_descriptor_list[1:]:\n","      train_descriptor_array = np.vstack((train_descriptor_array, des))\n","    \n","    test_descriptor_array = np.array(test_descriptor_list[0])\n","    for des in test_descriptor_list[1:]:\n","      test_descriptor_array = np.vstack((test_descriptor_array, des)) \n","\n","    return train_descriptor_list, train_descriptor_array, test_descriptor_list, test_descriptor_array\n","# pred2, label2 = # train_and_test(..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctu8ESjkoY32"},"source":["# clustering the descriptors using KMeans\n","def cluster_descriptors(train_descriptor_array, test_descriptor_array, n_clusters = 50):\n","    kmeans_model = KMeans(n_clusters = n_clusters)\n","    train_sift_centroids = kmeans_model.fit_predict(train_descriptor_array)  \n","    test_sift_centroids = kmeans_model.predict(test_descriptor_array)\n","\n","    return train_sift_centroids, test_sift_centroids"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N_MxIOtf8l6S"},"source":["from sklearn.preprocessing import StandardScaler   \n","\n","# generating histograms from descriptor cluster labels\n","def generate_sift_histograms(train_data, train_descriptor_array, train_descriptor_list, test_data, test_descriptor_array, test_descriptor_list, n_clusters=50):\n","    # clustering the descriptors using KMeans\n","    kmeans_model = KMeans(n_clusters = n_clusters)\n","    train_sift_centroids = kmeans_model.fit_predict(train_descriptor_array)  \n","    test_sift_centroids = kmeans_model.predict(test_descriptor_array)\n","    \n","    image_count = train_data.shape[0]\n","    # Training histogram\n","    train_histogram = np.array([np.zeros(n_clusters) for i in range(image_count)])\n","    old_count = 0\n","    # looping over the images\n","    for i in range(image_count):\n","      image_desc_count = len(train_descriptor_list[i])\n","      # updating the histogram array\n","      for j in range(image_desc_count):\n","        idx = train_sift_centroids[old_count + j]\n","        train_histogram[i][idx] = train_histogram[i][idx] + 1\n","      old_count = old_count + image_desc_count\n","\n","    image_count = test_data.shape[0]\n","    # Testing histogram\n","    test_histogram = np.array([np.zeros(n_clusters) for i in range(image_count)])\n","    old_count = 0\n","    # looping over the images\n","    for i in range(image_count):\n","      image_desc_count = len(test_descriptor_list[i])\n","      # updating the histogram array\n","      for j in range(image_desc_count):\n","        idx = test_sift_centroids[old_count + j]\n","        test_histogram[i][idx] = test_histogram[i][idx] + 1\n","      old_count = old_count + image_desc_count\n","\n","    # Normalizing the values\n","    scale = StandardScaler().fit(train_histogram)\n","    train_histogram = scale.transform(train_histogram)\n","    scale = StandardScaler().fit(test_histogram)\n","    test_histogram = scale.transform(test_histogram)\n","    \n","    return train_histogram, test_histogram, kmeans_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Fo_16Y4ujVC","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571886056280,"user_tz":180,"elapsed":552,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"667a3bb9-7016-4dc0-e1ac-943e6c205014"},"source":["from sklearn.neighbors import NearestNeighbors\n","from sklearn.metrics import accuracy_score\n","\n","keypoints = generate_keypoints(train_data[0], 10)\n","train_descriptor_list, train_descriptor_array, test_descriptor_list, test_descriptor_array = extract_sift_descriptors(train_data, test_data, keypoints)\n","train_histogram, test_histogram, kmeans_model = generate_sift_histograms(train_data, train_descriptor_array, train_descriptor_list, test_data, test_descriptor_array, test_descriptor_list, 50)\n","\n","# Initializing the model\n","n_neighbors = 48\n","model = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='ball_tree', weights= 'distance')\n","\n","# training the model\n","model.fit(train_histogram, train_label)\n","# prediction using the trained model\n","pred2 = model.predict(test_histogram)\n","label2 = test_label\n","prediction_accuracy = accuracy_score(pred2, test_label)\n","\n","# accuracy of the model\n","print(\"Accuracy: {:.2f}%\".format(prediction_accuracy * 100))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 54.50%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"ein.tags":"worksheet-0","id":"Z5cHPS2LL4mL"},"source":["##Bag of SIFT Representation + one-vs-all SVMs\n","\n","We train 16 binary, one-vs-all SVMs (ex: 'forest' vs 'non-forest')."]},{"cell_type":"code","metadata":{"id":"HO021G583XVo","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1571885938937,"user_tz":180,"elapsed":1301,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"b3ad1400-6e3f-430d-f8ab-364f2b6708e8"},"source":["from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.svm import LinearSVC\n","\n","def predict_SVM(train_histogram, train_label, test_histogram, test_label, C = 0.0068, max_iter=20000):\n","    # getting unique label and its count\n","    unique_labels = np.unique(train_label)\n","    num_labels = unique_labels.shape[0]\n","\n","    train_size = train_histogram.shape[0]\n","    test_size = test_histogram.shape[0]\n","    model_list = dict.fromkeys(unique_labels)\n","\n","    # initializing and training binary SVM classifiers\n","    for i in range(num_labels):\n","        category = unique_labels[i]\n","        category_indices = train_label != category\n","        labels = np.ones(train_size)\n","        labels[ category_indices] = 0\n","        model = LinearSVC(C=C, max_iter=max_iter)\n","        model.fit(train_histogram, labels)\n","        # storing the models\n","        model_list[category] = model\n","\n","    # prediction on the test data    \n","    predict_labels = []\n","    for i in range(test_size):\n","      score_dict = dict.fromkeys(unique_labels)\n","      for j in unique_labels:\n","        score_dict[j] = model_list[j].decision_function(test_histogram[i].reshape(1,test_histogram[i].shape[0]))[0]\n","      # selecting the label with max votes\n","      predict_labels.append(sorted(score_dict.items(), key=lambda score_dict: score_dict[1], reverse=True)[0][0])  \n","    \n","    return predict_labels\n","    \n","# training the SVM classifier\n","predict_labels = predict_SVM(train_histogram, train_label, test_histogram, test_label)\n","# calculating the prediction accuracy\n","prediction_accuracy = accuracy_score(np.asarray(predict_labels), test_label)\n","pred3 = predict_labels\n","label3 = test_label\n","# accuracy of the model\n","print(\"Accuracy: {:.2f}%\".format(prediction_accuracy * 100))   \n","\n","# pred3, label3 = # train_and_test(..."],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy: 65.25%\n"],"name":"stdout"}]}]}