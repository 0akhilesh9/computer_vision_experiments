{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Copy of CSE527_HW4_Fall19.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"XSfnuObtYYMH"},"source":["## Description\n","---\n","Train a deep convolutional network from scratch to recognize scenes\n","\n","(vs)\n","\n","Fine-tune a pre-trained deep network. \n"]},{"cell_type":"code","metadata":{"id":"0UA6WFgcYYMI"},"source":["# import packages here\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import glob\n","import random \n","import time\n","\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0NxnBcUqk_9B","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1573033094379,"user_tz":300,"elapsed":1479,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"a1ffc053-9b7f-4b36-f98f-8b861ba4f905"},"source":["# ==========================================\n","#    Load Training Data and Testing Data\n","# ==========================================\n","from scipy import ndarray\n","import skimage as sk\n","from skimage import transform\n","from skimage import util\n","\n","class_names = [name[13:] for name in glob.glob('./data/train/*')]\n","class_names = dict(zip(range(len(class_names)), class_names))\n","print(\"class_names: %s \" % class_names)\n","n_train_samples = 150\n","n_test_samples = 50\n","\n","\n","def img_norm(img):\n","  #\n","  # normalize img pixels to [-1, 1]\n","  #\n","  img = img.astype('float64')\n","  # Normalizing the image pixels\n","  tmp_img = 2.*(img - np.min(img))/np.ptp(img)-1\n","  return tmp_img\n","\n","# Image augmentation (mirroring, rotating, adding noise)\n","def img_augment(data,labels, mirror=False, rotate=False, noise=False):\n","  tmp_len = len(data)\n","  # Loop over all the images\n","  for i in range(tmp_len):\n","    if mirror:\n","      # Reverse the pixel values\n","      data.append(data[i][:, ::-1])\n","      labels.append(labels[i])\n","\n","    if rotate:\n","      # A random angle of rotation between 20% on the left and 20% on the right\n","      random_angle = random.uniform(-20, 20)\n","      # Rotate the image\n","      tmp = sk.transform.rotate(data[i], random_angle)\n","      data.append(tmp)\n","      labels.append(labels[i])\n","\n","    if noise:\n","      # Add some random noise\n","      data.append(sk.util.random_noise(data[i]))\n","      labels.append(labels[i])\n","  # Return the augmented data\n","  return data, labels\n","\n","# Noramalize the image (zero centering)\n","def img_normalize(data):\n","  tmp_data = np.stack( data, axis=0 )\n","  # Calculate the mean value\n","  tmp_batch_mean = np.mean(tmp_data, axis = 0)\n","  # Calculate the std\n","  tmp_batch_std = np.std(tmp_data, axis = 0)\n","  # (pixel value - mean) / std\n","  data = tmp_data - tmp_batch_mean\n","  data = data / tmp_batch_std\n","  return data\n","\n","def load_dataset(path, img_size, num_per_class=-1, batch_num=1, shuffle=False, augment=False, is_color=False,\n","                rotate_90=False, zero_centered=False, aug_list=[True,False,False]):\n","    \n","    data = []\n","    labels = []\n","    \n","    if is_color:\n","        channel_num = 3\n","    else:\n","        channel_num = 1\n","        \n","    # read images and resizing\n","    for id, class_name in class_names.items():\n","        print(\"Loading images from class: %s\" % id)\n","        img_path_class = glob.glob(path + class_name + '/*.jpg')\n","        if num_per_class > 0:\n","            img_path_class = img_path_class[:num_per_class]\n","        labels.extend([id]*len(img_path_class))\n","        for filename in img_path_class:\n","            if is_color:\n","                img = cv2.imread(filename)\n","            else:\n","                img = cv2.imread(filename, 0)\n","            \n","            # resize the image\n","            img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n","            \n","            if is_color:\n","                img = np.transpose(img, [2, 0, 1])\n","            \n","            # norm pixel values to [-1, 1]\n","            data.append(img_norm(img))\n","            \n","    #\n","    # Data Augmentation\n","    # mirroring\n","    #\n","    if augment:\n","      data, labels = img_augment(data, labels, *aug_list)\n","      # data, labels = img_augment(data, labels, mirror=True, rotate=True)\n","\n","    #\n","    # Data Normalization\n","    # norm data to zero-centered\n","    #\n","    if zero_centered:\n","      data = img_normalize(data)\n","\n","    # randomly permute (this step is important for training)\n","    if shuffle:\n","        bundle = list(zip(data, labels))\n","        random.shuffle(bundle)\n","        data, labels = zip(*bundle)\n","    \n","    # divide data into minibatches of TorchTensors\n","    if batch_num > 1:\n","        batch_data = []\n","        batch_labels = []\n","        \n","        print(len(data))\n","        print(batch_num)\n","        \n","        for i in range(int(len(data) / batch_num)):\n","            minibatch_d = data[i*batch_num: (i+1)*batch_num]\n","            minibatch_d = np.reshape(minibatch_d, (batch_num, channel_num, img_size[0], img_size[1]))\n","            batch_data.append(torch.from_numpy(minibatch_d))\n","\n","            minibatch_l = labels[i*batch_num: (i+1)*batch_num]\n","            batch_labels.append(torch.LongTensor(minibatch_l))\n","        data, labels = batch_data, batch_labels \n","    \n","    return zip(batch_data, batch_labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["class_names: {0: 'LivingRoom', 1: 'Suburb', 2: 'Mountain', 3: 'InsideCity', 4: 'Store', 5: 'Street', 6: 'Office', 7: 'Kitchen', 8: 'OpenCountry', 9: 'TallBuilding', 10: 'Industrial', 11: 'Bedroom', 12: 'Coast', 13: 'Flower', 14: 'Highway', 15: 'Forest', 16: '_DS_Store'} \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j5zIkA4zLjyt","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1573001010695,"user_tz":300,"elapsed":40926,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"f5c5c9fb-105d-43e6-c99f-4137810b9819"},"source":["# load data into size (64, 64)\n","img_size = (64, 64)\n","batch_num = 50 # training sample number per batch\n","\n","# load training dataset\n","# Data without augmentation and zero centering\n","trainloader_small_plain = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=False, zero_centered=False, aug_list=[True,False,False]))\n","# Data with augmentation\n","trainloader_small_aug = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=True, zero_centered=False, aug_list=[True,False,False]))\n","# Data with zero centering\n","trainloader_small_zero = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=False, zero_centered=True, aug_list=[True,False,False]))\n","# Data with augmentation and zero centering\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=True, zero_centered=True, aug_list=[True,False,False]))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches(=%d) of training samples.\" % (train_num, batch_num))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_num=batch_num))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches(=%d) of testing samples.\" % (test_num, batch_num))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","2400\n","50\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","4800\n","50\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","2400\n","50\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","4800\n","50\n","Finish loading 96 minibatches(=50) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","400\n","50\n","Finish loading 8 minibatches(=50) of testing samples.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wHI2VYbDloje"},"source":["# load data into size (64, 64)\n","img_size = (64, 64)\n","batch_num = 50 # training sample number per batch\n","\n","# load training dataset\n","trainloader_small = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=True, zero_centered=True))\n","train_num = len(trainloader_small)\n","print(\"Finish loading %d minibatches(=%d) of training samples.\" % (train_num, batch_num))\n","\n","# load testing dataset\n","testloader_small = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_num=batch_num))\n","test_num = len(testloader_small)\n","print(\"Finish loading %d minibatches(=%d) of testing samples.\" % (test_num, batch_num))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"krCXjDOzlq0f","colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"status":"ok","timestamp":1573033524959,"user_tz":300,"elapsed":549,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"5ce74001-90e7-4634-9302-1059a2342c0d"},"source":["# show some images\n","def imshow(img):\n","    img = img / 2 + 0.5     # unnormalize\n","    npimg = img.numpy()\n","    if len(npimg.shape) > 2:\n","        npimg = np.transpose(img, [1, 2, 0])\n","    plt.figure\n","    plt.imshow(npimg, 'gray')\n","    plt.show()\n","img, label = trainloader_small[0][0][11][0], trainloader_small[0][1][11]\n","label = int(np.array(label))\n","print(class_names[label])\n","imshow(img)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Coast\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD7CAYAAACscuKmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO19baxeVbXuM2gL/RJKodTalhZpBarQ\nApXyJXBAr/hFf2gMIiecKwkx6I3knhtFTcw5N/cmGhWPMfd40qj3NAYPeAAtQQLWCshXsPVSoLS0\nlFqktbCRtnxLaTvvj/2uxbOe/c6x197de72VNZ6k6XrfudZcc8255n6fZ4wxx7SUEgKBwNsfh/S6\nAYFAoBnEZA8EWoKY7IFASxCTPRBoCWKyBwItQUz2QKAlOKDJbmYXm9lGM9tsZteOVKMCgcDIw4br\nZzezMQA2AfgQgG0AVgP4TEpp/cg1LxAIjBTGHsC1ZwDYnFLaAgBmdgOApQCyk33q1Klp5syZXct6\nFdzTy6Ci0bj3wR4kZWaVzyPRXq2zyXYM594jgVx7d+zYgV27dnVt1IFM9pkAnqHP2wAscS+YORO3\n3HJL1zKvs+sOBJ+n1+Tq2L9//wHfd7jXDffe3nVcxsf6Uu7bty9blru3nue96LmyQw6pKkduI7dp\nsOsYY8e+9Rp7beI6Rmqy5+rU+vk59Vly763Xxtw7cNlll+Xbmi0ZIZjZVWa2xszW7Ny5c7RvFwgE\nMjiQX/btAGbT51md7ypIKS0DsAwATj755JT7Cz0SdI7/2tX9Zfd+CfQa7xc1V6fXDu2LvXv3lsd7\n9uwpj8eNG5etX6Hn5toxnF85ff7hMBP99fZ+rXK/2Py9lo0ZM6ZSxs/p3atuf+i9c+/zUJgU1/Hm\nm29m28jvh9f+HA7kl301gPlmdpyZHQrgUgC3HkB9gUBgFDHsX/aU0l4z+yKAOwGMAfCTlNLjI9ay\nQCAwojgQGo+U0u0Abh+htgQCgVHEAU32A8FQtHLd8zytXLcdng7N6SLvGu85WZcDVV3H2tN7zrrt\n92wHqnNzbVa9ndPDQFV7euPi9R1r1Lr9wdcAAzV2Dt55dT0G3A5Pb2sdfB336VDsCl67yusHPSMQ\nCLwtEJM9EGgJekbjPYx2lFVd2u3RRYbXPo9mezSeofLBc0Mpjc3V7dHFXF95LiMtGz9+fNd2eGPr\nBdwwjff6Y7jw3HJcP8sTvY7LvHdCx8gLpKl7XtFmV4ZmSwKBwNsKMdkDgZYgJnsg0BI0rtnrhGIO\nxeXAYF3qhXbWdTvVDY8dCvh+EyZMqJSxhn/jjTe6tkmhGjK34MIL89Tn5Dr5PO0rT2/n3GEazuvp\n0JxOH4pNp66e5/5WTe25/XJuOa+Nnu1juG7cog73nGxJIBB4WyEmeyDQEjRK4/fv34/XXnsNwEDa\n51Elrz4G02B1NbErqG5EVN3oLm0vP4snE5RyHXrooeUx02510XluIm6Xd686q6QGQ91IPu+8kQbT\ncQ867trHjJdeeqk8njhxYqWMJQ+P9VDW/teFN2ZFf3tzJ37ZA4GWICZ7INAS9Mwar3QjRz+1zKOw\nXvKKHE1TOufR+ty9veg0BVO9ulZwb+GEPlfOaq1tYrp72GGHVcq4v9lj4LU3lzQD8KklyxWvfk/m\ncR94EW51Lfra33xvrb9Oe7XOuouXtA4+Tz05dRC/7IFASxCTPRBoCWKyBwItQaOaPaVUK7mCaprc\naiIv0k5dMKwp+TyNlvJWOHm6i+HZH15//fXsdXWj/Lgdql+5zEsuwedpH+TsBV70mNoEWItz/d55\nXgJOT1N79o1c/bnVgd3KuE52j2q7+FjryK3g03ZxmddGfb+L9yxcb4FAICZ7INAWNErjDznkEEya\nNAnA0CghR7/lkgUAVcqsdCtHj5RScZmXn7xucgKVGiwnvMg7T9ZwHXVdQUPJ/Vb3Ob28bTl6OxR3\nKb8H3oIcz+WVo9aeBFS3lvfO5aLmtB3erjU5meC56HISMxbCBAKBmOyBQFsQkz0QaAkaD5ctdIe3\nGsx1HzgJGbxkCrmVaOqqqRv2OdzEFp4Wz+WN97Sb2ia4jpyto9tnhnfvHDy9zai7y+pg9TNy7kYg\nH46r4+ft58bwbDB187p7+915mr1O4hav7YP+spvZT8ysz8zW0XdTzWylmT3Z+f/IweoJBAK9RR0a\n/+8ALpbvrgWwKqU0H8CqzudAIHAQY1Aan1L6nZnNla+XArigc7wcwN0AvlLnhgUVGQoNziU/UCnA\nq5+Gsr1wXeSomEe3vHZoGVM4LxqLn9tL1uDRSq8fc24zBdfhrRask3Sh271y99b6vLHgd8J7ZoZH\nl72VaF4d/LluHj5v9Z3X/hyGa6CbnlLa0Tl+FsD0YdYTCAQawgFb41P/n6Xsn38zu8rM1pjZml27\ndh3o7QKBwDAxXGv8c2Y2I6W0w8xmAOjLnZhSWgZgGQCcfPLJqaBgSsU8upijekqV2BLpUUKP0nvJ\nJXI0ylvAMRQLc27hx1As6TkpU3enVq9+rw5vcYpHOb303zmvgCeNVNZwHfx+eO310mJ71v66iU+8\nfIDeQin2rgwl6Up5n0HP6I5bAVzROb4CwIph1hMIBBpCHdfbfwB4EMAJZrbNzK4E8E0AHzKzJwF8\nsPM5EAgcxKhjjf9MpuiiEW5LIBAYRfRsy2Yvwqju1jnelsqeTcDTmu6Wt04yCIZnO/DgJWFgeAkl\ncvnxPZ3rbZVcV2t6EXS8Qksj/jytzP3I7irV5d620rlVhp4u9/pK3WY5m8NQ8vLzdfzM3nbcXqRc\nDhEbHwi0BDHZA4GWoPEcdAUV8RYDKLxkEwzPbcaoG0lVl+JrhJtHfT26WHc3T28hT45KelF4Ho0f\nbpRcjp7XXYCj9+MypdJMaYebNz5Xn9bpXVd3B2Bvt1ovwQYjJzUiB10gEIjJHgi0BTHZA4GWoHHN\nXoRz1l09BFR1DLtdVFt5+4blVpTpXmncrr/+9a/ZNnmrtTz3IOs/z7XiuaQ85HKNq87l59Q+yCWx\nrOvO9DCU1Vqs2XncvdWOXhu9Mau7Zbhnn8m1XdvlJTn1xsULoS7qD80eCARisgcCbUHjEXQ5Gue5\neHL0Remm5yLJuZO8raa8bZH4Ol2pxFFsSisnTpyYbX+O6ul5daPfuEwpoZdMgT/XjQrz3Ig8ZiqN\nuK90LHgraabxw3Xf1U3E4UXQeVGEXhSotyUY18H9ozLPG4s6kZrxyx4ItAQx2QOBlqBRGr9//368\n8sorAPyEDErFctsdKd33rOC5hQNeVJjS25zVVynVa6+9lr1vLm0wkH82pZX8nJ51mOtQGs/t0oUl\ndampt/AoR591XDzvCveHR6W9BBI56ut5f+omUlF4Kbi9XWhzdXqeotyYhTU+EAjEZA8E2oKY7IFA\nS9CoZt+3bx9efvllAP4qKQVr51dffbU89rY+8vSwF/nlJXzIrSLT8zyd62lx1mjcfs+u4Ll4vDzj\nnksq5zbz7CB1k2J6kZN1Vw8ON889YyhbPHn2JIb3/tVdaelF+Xn9XdQZWzYHAoGY7IFAW9Aojd++\nfTuuvbZ/W7ijjjqqUrZz587y2NtJlen45MmTK2VelFIucs2jSlp/LupMJQiXKWXjqDClre94xzvK\n4xdffLFWHfqc3K/qusnd69lnn62UTZkypeu9tT88mcCRcV7+OKa7EyZMyLbXi3DjMeO+0et4rPU8\nho6L59LN1eO5zYa7ky27dHO7yYbrLRAIxGQPBNqCmOyBQEvQs7zxzz33XOUzaxzW10DVpcFatq+v\nusUc6yKtgzUO1/eXv/ylcl4ucQOQX5Wmq97q7hvmhV6ytlVdmFuVptd5NgyGl2jBWx2X61OFF1bL\ndUybNq1SxmNThFl3q4N1v5dY8+ijjy6PdfUdf/aSY3ih3Ax17U2dOjXbfh7fwjUNVDU6kN8TgOvc\nunVr1/YA9bZ/mm1md5nZejN73My+1Pl+qpmtNLMnO/8fOVhdgUCgd6hD4/cC+MeU0gIAZwL4gpkt\nAHAtgFUppfkAVnU+BwKBgxR19nrbAWBH5/hlM9sAYCaApQAu6Jy2HMDdAL7i1bV///6SLnlJI5iy\nAVXaM2nSpPJYKRVTWKWVL7zwQnnMtEndPUyV+F7aDm4vu6qAKpVUmsdlKhlytNtbJeXl4fO2k/Io\nfi46UM/j/tGIMaatu3fvLo91bLm9xx9/fKWMz83lEFTomOW2QNZx53dCJeCRR75FWg8//PBK2ZNP\nPlkee1LGc5sx+H3hdxHIb8cNDOz/bhiSgc7M5gI4FcBDAKZ3/hAAwLMApg+lrkAg0CxqT3Yzmwzg\nZgDXpJRe4rLU/6e2659bM7vKzNaY2Zo6G8YHAoHRQa3Jbmbj0D/Rr08p3dL5+jkzm9EpnwGgr9u1\nKaVlKaXFKaXFQ0kjHAgERhaDanbrF2o/BrAhpXQdFd0K4AoA3+z8v6LODQsd6YUrqiarmyyS/5h4\nWpa1oLINDedk8Lmst/VZWPOplmLtVnfbam8rYwXfjzWfunH4Xl793r24zNuKOdc+oKpDN27cmK3f\nCzP2wllZA/Mzq52FXW9aP7vDeNUlkNfKWj8/p4bS8vvNY6bn8XulbSyezd3DLlvyFs4B8PcAHjOz\ntZ3vvob+Sf5zM7sSwNMAPl2jrkAg0CPUscbfByD35+KikW1OIBAYLTQaQWdmJTXzcsN7+bI9eKu8\nmMbyeZ4L8Pnnn6+UcbuYUim1Yzef0i1vxV1uayh1wfD9VHbkcudrn+oKNkYuMk77iu+tLsZjjjmm\nPH7Xu95VHnsRaH/84x8rZdx3TOO9JJ76ruSSNLI7UM/Td5Mpvo5FLs+7riTMPYvCe4dz7QXeWsXo\n2cUiNj4QaAlisgcCLUHjNL6w0ioV8xIysGWXky5s3749W0duN1NgYIQUo66c8OiSUr1c/WqxZnru\nReExddQIMaagXsIEPk/7iqkk032l/jwWKmX4OXmcPPrsSR4vvzyPi/YV1+ElRanbH54lnS3zXv5C\nHTOOyuMIQO1Tb0uwQqZ6kjd+2QOBliAmeyDQEsRkDwRagsY1e7HaSFcnccJJ1cOs0dit5SX/89wb\n7IbzkkVqG1kPcRSeanQvsuyll95aVqD35tVVrMu9FU7s4gKqz811eAkqVIfm7BaqlTmRiI4F63nW\nwPw9UF19pv3I1/G9NfEEn+clEOWoNl19x+AkF0DebevdW5+F69Rkq6zNNZkKg8dQ7U7hegsEAiVi\nsgcCLUGjNP6YY47B1VdfDQDYvHlzpezPf/5zeTxnzpxK2apVq8pjpk0aPcZl6j7JubKUSjOt1MUj\nua2hlDoy1dN2zJo1qzzWKCumc+yeUcrGdWqU3xFHHIFuUPrMckgpJ9+bn00TPnz4wx8uj3ft2lUp\nW7t2bXnMffz+97+/ch738ZYtWyplLO24D7zoP5UaPBbcRi8hiLeVlefS9fLXs2xQ92NuW2m9F8s3\nXpzD9bs5FLMlgUDgbYWY7IFASxCTPRBoCazuFrcjgQkTJqQiqaCGb7I7Rd1VrLFZT6p2U43NqKu7\n2A6gfcNtrtteL5mAunFYb7EbTuv3klbmXF5aB+tErYO1J2tbdVdxH6gLkOv3crJzmfZVzl6gdXD7\nvXBRfi49zwv99RJ4LFmypDy+//77s9fwe6uuQ7aZeO5Bdsvlxr2vrw979uzp+tLFL3sg0BLEZA8E\nWoLGafy8efMADKQoTM2U5jC9Y+ru0U99LqaBXJ/nolOq9573vKc81ug6hm5txWDqztF0em923XhZ\neT1J4p3HcsXbbnn69LcyhKurkGWTt9rMy9fHn+tScK2Dn60uBfdyFHo52HUs+Ll5bPU8L5FILsei\n9qknJwo89thjeOWVV4LGBwJtRkz2QKAlaDSCbsyYMWWE1wc+8IFK2fr168tjL50zU0eN/GI6yhFi\nQJXeMaU/6aSTKud56Xovv/zy8pjzqj344IOV81avXl0eKxVjis/RdABwwgknlMe8yOSRRx6pnMeW\nY95+CACOPfbYrvfSyDWWQxr9tnDhwvL40UcfzbbX25KJIyJ5XDTyi++tEXos51gKeFtZaQrn3HZY\nKhm4Do2q5HaofONx8rYV477y0nrzdd72YIqi/yN5RSAQiMkeCLQFMdkDgZagUdfb+PHj09y5cwEM\ndL3xai2NjOPVT6yn1M3C2kqj03KuFdV/HOl03HHHVcre/e53dy3j7YkB4MILLyyPVWdt27atPFZ9\nya491nVPPPFE9rxf/epXlbKlS5eWx6yHVcv99re/LY+1H7n9jJkzZ1Y+T5s2rTzW/uYc8KxDdcUX\nJ3LQ/rjjjju63lu1LI+hru5jGwG/V+985zsr5z3zzDPlsdpgFi1aVB7re8vjyX3w2GOPVc7L5eIH\nqjYqHgvPNqEo7AWbNm3Ca6+9NjzXm5mNN7Pfm9kjZva4mf1z5/vjzOwhM9tsZjea2eAbRAcCgZ6h\nDo1/A8CFKaWFABYBuNjMzgTwLQDfSynNA7ALwJWj18xAIHCgqLPXWwJQcJdxnX8JwIUALut8vxzA\nPwH4oVfX+PHjKxSUwTm62A1XXFeAaVRup1DAz83NNFDpJ9PndevWZetnt5bSLV6woAtE2A2lLjWW\nVNx+pssAcPvtt5fH2lfsOmT3oN7rnnvu6dpeALj33nu7tvfEE0+snPfFL36xa3sB4PHHHy+POW+8\nRuuddtpp5bEm4uB3xUu2wfJNXa4s3/g5+/qqO4yz1Ni6dWuljN2bKnu5jdwOlp5AVU6oO5brZHmh\nkXwsQ3LJKw44B52Zjens4NoHYCWApwDsTikVImIbgJm56wOBQO9Ra7KnlPallBYBmAXgDAAnDnJJ\nCTO7yszWmNmaupvWBQKBkceQXG8ppd0A7gJwFoApZlZwpFkAtmeuWZZSWpxSWuwtMAgEAqOLQV1v\nZjYNwJsppd1mNgHAr9FvnLsCwM0ppRvM7N8APJpS+levrvnz56frrrsOwEBXELs3NNRw06ZN5THr\nndmzZ1fO++Uvf1keFy6+AqyFli9fXh6zeweouozOPvvsShm7SFhfquZlnagJIIvkHcBA3cjuH9bp\nutopt58bULUR5FaeAVXXkOZJX7BgQXnshSdzH6vOzYXtajJR7jsNk2aXJtsAPDvIjh07KmXssvMS\ngeaSbQDV/mZXG5C3J2loMbsEvXBZTrqpz8nvO4cjA2/16z333IPdu3d3db3ViY2fAWC5mY1BPxP4\neUrpNjNbD+AGM/tfAB4G8OMadQUCgR6hjjX+UQCndvl+C/r1eyAQ+BtAoxF08+bNS9/97ncB+PRT\naSWDaZ+6gjjS7E9/+lOljGUCUz11a3HyAKaOej92cWkyBaZ2GknFCSs0Mo5dVLwCTqO2mAaqXGH6\nzLRY6SdDKT7XyXRRIwUZ6gJkWs/31hV2LNk06QePtZf8gWn3KaecUilj2ceySd8/ptIqMbm/NcqP\nx4bbpVGgLPvU9cZuOpYampOP+0rf2+J5nnjiCbz66quRvCIQaDNisgcCLUGjySv2799fBvp/8IMf\nrJRxBJZGQXGkHNOtp556qnIeSxKVAkwlWTLMmDGjch5TQo5AUzD11civ9773vV2PgSpNU6rH7WK6\ne/LJJ1fOY48B03agSivvu+++8ljdnqeffnp5/O1vf7tSxpZklis33XRT5bxzzjmnPNbtvDjCi8dM\nreCeZ4HHnamvSk9uo0ojvi63zVK3OhmcsEIX8rCFnN9blZH8bBptyGnDud90UQ8vktEIvaIfY/un\nQCAQkz0QaAtisgcCLUGjrrepU6emYptfXoUGAEU+eWCgdmO9ybqLtStQdXlpogV1dxTQBIKsoznZ\nIlDVl5y8Qm0HrAc11zo/i7rDuP7zzz+/POYEkEB1NZ665VgTs31A3ZQ/+9nPymMvz/38+fPLY3Vr\nbdy4sTxWHcrP5q1K42f2tlHmMnW98TN7ed35Om+dhrpSeTxV6+dy+qttImc70DpZc6vrje+lbSzc\nrLt378bevXvD9RYItBkx2QOBlqBRGj9nzpz0ta99DcBAGsKuMqXnnJub88B5ObJ58QxQpZVM1Zku\nA9UoJXVvsCuEaau6tTjySxdVcBSXLizhaDWmdtoOfm6NamOqxy60DRs2VM7jSDDdhioXHajUl6WX\nUmuWYry4QyPQGLoAivubx08lGbvDPJqdkxb6WceTP3suTH6HlcaznOP3Gaj2P4+tzhF2x+ocKebF\nypUrsXPnzqDxgUCbEZM9EGgJYrIHAi1Bo+Gye/bsKbWG6j/WshpGytqTXXaavIJ1l+o/TsjA2ke1\nLK8m0jrYbcShixouy0koVKNy8gZ9ziVLlpTHbBPQJAZctnLlSuTA7dc86ayH1XbA92PXpD4nP5tq\nYHZHsg1DXaLcH5p4gjXwihUrymMNWWWdqyGmrLG5TMOpOexY3cLc3+qqffrpp8tj1uJszwCq77e2\nn/uO90BU+wDbATQ5RhFyyyHSivhlDwRagpjsgUBL0Kjr7bDDDkvFSjLdWomTNSjlZHrH1FdXrDHN\nUarE7h92x/D2Q0CV4mtONKZwHFmmdXBU2913310p42fRbZTZXchSQN1aTEd162tOlsF18Ao1APjE\nJz5RHqtc4c/e+8HUVN2gTIVZFmjUIFNfTcTB7eC+1xz4vNKNnxmovkvcb7zSDMhvD6bXaa497gN2\nCT7wwAOV89h9On369Oy9uR81N/xZZ51VHuscKepfvnw5duzYEa63QKDNiMkeCLQEjeeg+853vgNg\nIDVlyqwRUlymUUU5aFQYUzNe+KGLbtgyqhZmtiqzNV5pvJcumlNL66IQbrPem8HWYS9fH8sCpX0s\nNXRLLqbgbHFXms3Pou1lOnrnnXeWxzruXIcupmEay/fmCD+gmiNOpR1LNqbPGmG5du3a8lj7g985\n9Rhceuml5TGPhVrFWVZqH7A0ZVmjngWuQxfgFHV+/etfx5YtW4LGBwJtRkz2QKAliMkeCLQEjUbQ\njRs3rtRNqstZ26ouZ+3C2+d6Cfl026VcIj+NCuPEiar/OPqLdb+3PRO3CaiubGNtD1T1MbuGdHsp\n1tuaYJA133nnnVceL168uHKeZwdhOw7bOvQ5f/Ob32TbwXYMb5tt1tuaPJP7h+t4+OGHK+exPUJX\ng/FYs/tVk3l86lOfKo85fz9QtSXoKkN2A7KrUG1G3Kdq32AbBNenK+f4PVC7xfve9z4A+WQawBB+\n2TvbNj9sZrd1Ph9nZg+Z2WYzu9HMYtfGQOAgxlBo/JcAcCD5twB8L6U0D8AuAFeOZMMCgcDIohaN\nN7NZAD4G4H8D+O/WnxzrQgCXdU5ZDuCfAPzQq2fPnj1lBJUuAmHKrK4spk6e641pmkbQMTVj15K6\nHjkiTSk4u1Y4iYbmhudto5T2cRIDXczAUWLcRpUrTH21HzVvWQGNkmPXmC604XN5QYu6KT/2sY+V\nxzouHDHGbTzppJMq53HEoko7zr3O99YINN5tV98dztd30UUXdb2vtlclIC/a0jx8DB5bddHxGGo/\nskzg/lH5dtppp2XvXUgslVqMur/s/wLgywCKN+QoALtTSoWo2AZgZrcLA4HAwYFBJ7uZfRxAX0rp\nD8O5gZldZWZrzGyNGi0CgUBzqEPjzwFwiZl9FMB4AIcD+D6AKWY2tvPrPgvA9m4Xp5SWAVgGAMcf\nf3xz4XqBQKCCIYXLmtkFAP5HSunjZvafAG5OKd1gZv8G4NGU0r9615944onpRz/6EYCBCQJYJ911\n112VMtaXvO+Z5jvXhAEMzpPOK9Z4K2egqi9V13FIJesu1Yms11SzeyGmnESCk2qotmf7hmpl7it2\nm6k7id06aptgewSzMe1vvre63lhXc4IHzjUPVPvxjDPOqJSxu4375tRTT62cxy5ATRbCYbZsx9GV\nc1y/2jDYlqDvLWtkfk7V/dxX+p5yH3hba3OZ2i0KO8vnPvc5bNiwYcTDZb+CfmPdZvRr+B8fQF2B\nQGCUMaSgmpTS3QDu7hxvAXCGd34gEDh40GgE3euvv14mV9DkAUytP/nJT1bKmEZxcgJdNcYrmXQ1\n2DXXXFMes2vswQcfrJzHtFUpMlNVTiSg7WBqp6veGJqcgN1m7E7SKD9uI+fW0zKmt+yuA6r0Vt11\nLFeYSutKsc9//vPlsa5Ye+ihh8pjlkp6HtevW2UxZWb5polPPvvZz3ZtO1CVPOzW0hV87OZTdyaP\nkxqZed8BPk/pPksI3d6atwbnsVAXHUsU3UugkA0jEkEXCAT+thGTPRBoCRpPJV1YLO+5555KGdMq\nzc3GkUO8gEN3WWUqximhgSo1Y2urWtzZ4ql1MCXkvGdshQWqFE5pHy9I0R1YeZEIR0/pAhF+bs39\nxhFjnC9NF8IwFdb0yGvWrCmPuU857TMAXH/99eWx0vOPfOQjXct0oQpbxZVaX3zxxeUx01vOswcA\nP/3pT8vjb3zjG5UyjgbkJBpqtWe5pVSYE1FoH/BYc/SlbvHE1nl9X1gu8rupsok9O9rfxf2CxgcC\ngZjsgUBbEJM9EGgJGk04OXHixFTkh9cII9ZWGo3FkWCshdT9wNqHXTUK1jsaScXuE01ewS41Pk+1\nPSdTUM3OCTDWr19fKWNdyqukLrnkksp5/Ny6uo/1N2s+dUmxHeAHP/hBpYztFjfeeGN5zPYAvbeO\nJ/cJ21x0bDkJg67qYvcpvx+a153tCproke/H2yvr6juODtTc87zFFicE0XYtXLiwPNa+Yi2tK+f4\nPfC21OLEJ2pzKJJxXH755Vi/fn0knAwE2oyY7IFAS9AojZ8yZUo699xzAQyM6GL6qRSFXVQcTaeR\nTuz64AguoBrlxskIlIJ70W+8eIRddppcgqmj5hHjKDGNsmJKyFJDc9Vx+zXqjCku16HJK7i/NYKO\npQe3XxMj8LhoNCD3CT+nSi9uo7qT+N1k+aN9yu3XOvi6M888szxmSqzXccIOoPrcOl+YkrOU0QVW\nN910U3msUob7h3PWqxRlyaOJLQqX3dVXX42NGzcGjQ8E2oyY7IFASxCTPRBoCRoNl923b1+p7VSz\ns47RZAocJshJBnT7XE74oGGk/JlXE6nrijWvlvFnbodqSLYl6HOyW05XNXGdfC9OpKl1FPnCu92P\nc+xrnnHuY20j61IOT1Ztz0/LZikAAA/zSURBVH2qySLZDaohyQy2mWj93K98nvY322PUjsO2ihUr\nVpTH3NdA1c2lNhjuD7039wHbATQEmd2I6pZbsmRJecz2AU1ayQlI1NZU9IHaLBjxyx4ItAQx2QOB\nlqBR19sRRxyRiqQP3vZMmqyBKSFHImkd/CzqmuA8cezqUCrt0dtcTjeljpyYQ91rLDWUSurnAion\nOP+YuryYtnJ9mj+OV1dpG3PbVms0oJeDjlfwcbSatpepqkaM8XVcpivKuE6tn8eXKbLKGqbZOg78\nLNpGfuf4WVTWcN9pjjuWOdxGvRdLg9x+Affeey92794drrdAoM2IyR4ItASNWuNTSmX0l1JkppJs\nbQaqkVpMldTiznUoVWIrJVuYdbE/032OvgKqVIzvpZKB26WWaI6GU5rGn3OLQIBqBJ1ah7lfeUGH\nUlM+T8eCZQhLJW+TD6WVTJ/Za6KLWDhNttJzTqLB7eDxA6rPpvSZ351iERZQzfsGVJOR8MIaoPos\n+l6x5Z7HRcFjqzKVr+Nn0R1vuUw9FwXF12sY8cseCLQEMdkDgZYgJnsg0BI0rtkLPas6V6PmGKwV\nWauo5mWtrNFHrMXZTaQ6i69TPcx6iO+lOoldZera5OfUbaP4eVhrarQU61J1NfEqOM7Fr66x3H2B\nvG1Co8LYlqC2D9bms2fPLo/VjchRZ170F9sEVPezjUdtJHwda151ubItxesrtSex/mY7wFC2bM5F\n76ltgs/T7Z+KdngJJ+vuz74VwMsA9gHYm1JabGZTAdwIYC6ArQA+nVLKx0UGAoGeYig0/u9SSotS\nSkVO4msBrEopzQewqvM5EAgcpKgVQdf5ZV+cUvoLfbcRwAUppR1mNgPA3SmlE3J1AMCkSZNSkStL\nXQe82EWTJChVLaAuDK2TwdFeXL9SR6bkShf5M1NC3WqKabDSVo7c8sq4Du0Pdo2pFODnZIqs7jU+\nTxd+sNRgyeAtAlEZxs/CbjN937h+lVQsE7h+jZzkXIHajlwufqXqTPe1Du4rz7XFdeieAEzrVQ4x\nred3WPuKr+OddoG3FsLcfPPN6OvrO6AIugTg12b2BzO7qvPd9JRS8QTPApje/dJAIHAwoK6B7tyU\n0nYzOwbASjN7ggtTSsnMulKEzh+Hq4CBv5SBQKA51PplTylt7/zfB+AX6N+q+bkOfUfn/67blaaU\nlqWUFqeUFisdDQQCzWFQzW5mkwAcklJ6uXO8EsD/BHARgBdSSt80s2sBTE0pfdmra9q0aWnp0qUA\nBoa6sstIVyTxHwnWU9p21kwaesnalvWrajd1izByLikNI+X2ahgph7CqG4e1IbdRkzTyc6veZs3H\n7VJXjZckQbV57vvceUC1H1l7azJR1tHsKgSqGpX7RpOWcP3qpsyFWnNufKBqd9FklPyuarIQ7bsC\nuiU5P4uGLvMKSu5TfXd4zNTFVpTdf//9ePHFF7tq9jo/tdMB/KLzEo0F8LOU0h1mthrAz83sSgBP\nA/h0jboCgUCPMOhkTyltAbCwy/cvoP/XPRAI/A2g0eQVhx9+eCq2DlYazzRKI7qYmnKElLpImDqp\ne4PpHB/rFk9MmdWVx/KCy1QycB26CovbqGVswOQ2KmVjd6FKAXYNcbtUarAMUVsKU3Buk7qduB3e\ntk481poog6GuSG4zR5NxRB5QfXe0DnbTcT+qm4/bX7iHC6xevbo81i2nWZawVFJjNL+3+n7zGPKz\nae5BbrO+34U0WLVqFXbu3BnJKwKBNiMmeyDQEsRkDwRagkY1++TJk1Oxf5VqZdZMGsKac/GoLsqt\njgOqrixPh7K+VLsCu0y4zMsWo25E1mvqgmGtzPYIdQfys6l24/rnzZtXHqtmZw2pmXC4T7j9Xv53\n1f3cP164KWeqUXcVu9hYb6utwwtx5v7gOtTew2Ooep77QO/N17E9Qles8Thp/fy+cB06DzhsXOsv\nbBP33XdfJJwMBNqOmOyBQEvQs4STuirIc0nlthnS1XBMt5TOzZ07tzzWCCwG012mn9pmpoRKx7nM\ny6eubWR6XncdgW7ZzJQ8F5kFVN1EmlOe780RYl4iDq2f6SjTVnU7cZlGCnL/87h7q9I06iyXcMRL\nWqnPwuOuFJzr9JKisHtQn5Pv9/TTT5fH+v5xm+fMmVMpKyIzveQV8cseCLQEMdkDgZagURo/efJk\nnH/++V3LPEspW3qZHnmWbqWcTJV4UYxa3NUizMhZ+9VqyhRZ62earXSOsXDhgAjlEkzPTz/99ErZ\nAw88UB7zopC+vuqiRM6XplKArf9erj3uU/UY8HX8zEqRuV1q0Wdq7W2plduuCsjnoFOZxGVe/neN\nruMdZD1px+dp/Rz1pzKEwR4sraNYEBV54wOBQEz2QKAtiMkeCLQEjWr2cePGlXpWI9z4s+pc1iF8\nXm6LY2CgC4LPZReJ1sH3Vh3KdfKx6lBuo6eh9Dm5Tr5Ok0UuWLAg28YiQlHbpW3k69gtqfdjt5PW\nwXYRbSNrSl55plF43FeqQ9ndxi5A3W+NbR+6Px8nx2CbgNpmOGGFJvHkMnXb8hhy/ZoUk3W5vnPH\nHntseexFWLKtQlf3FeeGZg8EAjHZA4G2oFEaP2HChJKC5nJoAQOjmxhMwZUGMyXU+pneMB3Ve+Ui\nrrR+jm5SNw7TW29bJG9RBfeHuhi5jUrjuQ5uhz4LX6dRfkwX+d6e683rK65f6+DzVCZwH3AdKgF5\nDNUNyn3MFFlpMCeh8HLse1tTcztUJvC91XW4YcOG8vjss88uj3/3u99VzuNn0b0KCqnhLWyLX/ZA\noCWIyR4ItAQx2QOBlqBRzT527NharjfVsuySya0yUniuprp1aBnrb6+O3Co9oPosqrdZb7Je9RJs\nKHLXaRu5XV5yDIauVGR9qCu0ci4pHRdvdR/XmdurT9uh4PvxM2sfen2ae/+Aqk2D7+W939r+U045\npTzmsWCXHOC71QqbwLp167LnxC97INASxGQPBFqCRmn8/v37yxVtGi2Vy+8GVGkaHw+lDs8tx/Ao\nJ1M2bodSNqZiWgdTSaWfuVV7nrtK783XsStIqWPORad1srTwtrDW+nMJPPRePE5ePzJUGnEbvVz/\n3I9eAgxto7dHYc4dq2PGY+HJJm8La+/dGbEIOjObYmY3mdkTZrbBzM4ys6lmttLMnuz8n18bGggE\neo66NP77AO5IKZ2I/q2gNgC4FsCqlNJ8AKs6nwOBwEGKQWm8mR0B4DwA/wAAKaU9APaY2VIAF3RO\nWw7gbgBf8erat29fmTdOI5G8rXlyllL9nq3FSjlzVnC1MDPd8tIS83VqidbPufo9esjPpv3BtE/T\nQDPV8/qUaaA+J9+byzwPh9JH/uzRW0+ucB9rymyG10a+t5eOmuFZ5vW6nCT0ZIIiJ8t0QY4XiVi8\nS94uxHV+2Y8D8DyA/2tmD5vZjzpbN09PKRVLkZ5F/26vgUDgIEWdyT4WwGkAfphSOhXAqxDKnvr/\nPHd1dprZVWa2xszW6EYFgUCgOdSZ7NsAbEspPdT5fBP6J/9zZjYDADr/93W7OKW0LKW0OKW02NvB\nMxAIjC7q7M/+rJk9Y2YnpJQ2on9P9vWdf1cA+Gbn/xWD1TV+/PhySyLVkKxBdNE+MwIuU6bgRadt\n27atPObEfWo78LYyZn3maUgv+o01mSZH5Po9Hcrw3DNeskgvaQTfm8tUd/Jz6rPkEoRokk1ebabj\n6SWqZHCZtjG3SlK1tqepPX3PbWR7jK5U9GxBbLfw7EncRu3vYiy8aMK6fvb/BuB6MzsUwBYA/xX9\nrODnZnYlgKcBfLpmXYFAoAeoNdlTSmsBLO5SdNHINicQCIwWGo2ge/PNNwfkKC/Ai/HVbcGUi5MO\neNtEKQU/4YQTutav7g2mW0p9NalBAaWYTO10AQrTYk2gwHnLmNKqZOBtr5S2cR52livPP/985Txu\nl/YBU0mmnB5F1Mg1vo7pqI4LR7+5FJSuU9eYl/iE+46fS2k8X+dFWOp45hYv6TvMOfy9XYr5HVM5\ny+9OkSe+W1kOERsfCLQEMdkDgZYgJnsg0BI0qtnNrNQ1uvdYseUsMDDndm4rZtVPrLtUu7GrgrW4\nalnWT5xzHKhqMs5drm6b3L0UGurKmpXtEeqSyiXPBKquLa5fNbWXc9+zKzB4XLwkjd59ve2QWduy\n5lXNzu+BvhNcP/dp3S2xgeq7pC4vHgt222obeQzV1sR1cJk+y5YtW8rjTZs2VcoKW5i+s5X7ZEsC\ngcDbCjHZA4GWwDx3x4jfzOx59AfgHA0gzzeawcHQBiDaoYh2VDHUdsxJKU3rVtDoZC9varYmpdQt\nSKdVbYh2RDuabEfQ+ECgJYjJHgi0BL2a7Mt6dF/GwdAGINqhiHZUMWLt6IlmDwQCzSNofCDQEjQ6\n2c3sYjPbaGabzayxbLRm9hMz6zOzdfRd46mwzWy2md1lZuvN7HEz+1Iv2mJm483s92b2SKcd/9z5\n/jgze6gzPjd28heMOsxsTCe/4W29aoeZbTWzx8xsrZmt6XzXi3dk1NK2NzbZzWwMgP8D4CMAFgD4\njJktaOj2/w7gYvmuF6mw9wL4x5TSAgBnAvhCpw+abssbAC5MKS0EsAjAxWZ2JoBvAfheSmkegF0A\nrhzldhT4EvrTkxfoVTv+LqW0iFxdvXhHRi9te0qpkX8AzgJwJ33+KoCvNnj/uQDW0eeNAGZ0jmcA\n2NhUW6gNKwB8qJdtATARwP8DsAT9wRtju43XKN5/VucFvhDAbQCsR+3YCuBo+a7RcQFwBIA/omNL\nG+l2NEnjZwJ4hj5v63zXK/Q0FbaZzQVwKoCHetGWDnVei/5EoSsBPAVgd0qpWFnT1Pj8C4AvAyhW\njhzVo3YkAL82sz+Y2VWd75oel1FN2x4GOvipsEcDZjYZwM0ArkkpVZa0NdWWlNK+lNIi9P+yngHg\nxNG+p8LMPg6gL6X0h6bv3QXnppROQ7/M/IKZnceFDY3LAaVtHwxNTvbtAGbT51md73qFWqmwRxpm\nNg79E/36lNItvWwLAKSUdgO4C/10eYqZFet4mxifcwBcYmZbAdyAfir//R60Ayml7Z3/+wD8Av1/\nAJselwNK2z4YmpzsqwHM71haDwVwKYBbG7y/4lb0p8AGaqbCPlBY/6LyHwPYkFK6rldtMbNpZjal\nczwB/XaDDeif9J9qqh0ppa+mlGallOai/334bUrps023w8wmmdk7imMA/wXAOjQ8LimlZwE8Y2ZF\nwsQibfvItGO0DR9iaPgogE3o14dfb/C+/wFgB4A30f/X80r0a8NVAJ4E8BsAUxtox7nop2CPAljb\n+ffRptsC4BQAD3fasQ7ANzrfvxvA7wFsBvCfAA5rcIwuAHBbL9rRud8jnX+PF+9mj96RRQDWdMbm\nlwCOHKl2RARdINAShIEuEGgJYrIHAi1BTPZAoCWIyR4ItAQx2QOBliAmeyDQEsRkDwRagpjsgUBL\n8P8BE8PDlUVDodQAAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JGA-K6QzYYMR"},"source":["## Training a Network From Scratch:"]},{"cell_type":"code","metadata":{"id":"rsygR1lm6Cm9"},"source":["# ==========================================\n","#       Define Network Architecture\n","# ==========================================\n","class CNN_base(nn.Module):\n","    def __init__(self):\n","        super(CNN_base,self).__init__()\n","        l1_channels = 8\n","        l2_channels = 32\n","        self.seq_model = nn.Sequential(\n","          # Convolution layer 1 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=1, out_channels=l1_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l1_channels, out_channels=l2_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.linear_layers = nn.Sequential(\n","          # nn.Dropout(p=0.5, inplace=False),\n","          # Linear layer\n","          nn.Linear(in_features=l2_channels*16*16, out_features=1000),   #Flattened image is fed into linear NN and reduced to half size\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Linear final output layer with 16 output features\n","          nn.Linear(in_features=1000, out_features=16)      #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n","        )\n","    # Forward pass\n","    def forward(self, inputs):\n","      # Getting output from convolution layers\n","      output1 = self.seq_model(inputs)\n","      # Reshaping (flattening) to feed into the subsequent linear layers\n","      output1 = output1.view(output1.size()[0],-1)\n","      # Getting output from final linear layers\n","      outputs = self.linear_layers(output1)\n","      return outputs\n","\n","class CNN_mod1(nn.Module):\n","    def __init__(self):\n","        super(CNN_mod1,self).__init__()\n","        l1_channels = 8\n","        l2_channels = 32\n","        # Convolution layers\n","        self.seq_model = nn.Sequential(\n","          # Convolution layer 1 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=1, out_channels=l1_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","          # Dropout layer\n","          nn.Dropout2d(p=0.5),\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l1_channels, out_channels=l2_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.linear_layers = nn.Sequential(\n","          # nn.Dropout(p=0.5, inplace=False),\n","          # Linear layer\n","          nn.Linear(in_features=l2_channels*16*16, out_features=1000),   #Flattened image is fed into linear NN and reduced to half size\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Linear final output layer with 16 output features\n","          nn.Linear(in_features=1000, out_features=16)      #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n","        )\n","          \n","    # Forward pass\n","    def forward(self, inputs):\n","      # Getting output from convolution layers\n","      output1 = self.seq_model(inputs)\n","      # Reshaping (flattening) to feed into the subsequent linear layers\n","      output1 = output1.view(output1.size()[0],-1)\n","      # Getting output from final linear layers\n","      outputs = self.linear_layers(output1)\n","      return outputs\n","\n","class CNN_act_fn(nn.Module):\n","    def __init__(self, activation_layer):\n","        super(CNN_act_fn,self).__init__()\n","        l1_channels = 8\n","        l2_channels = 32\n","        # Convolution layers\n","        self.seq_model = nn.Sequential(\n","          # Convolution layer 1 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=1, out_channels=l1_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          activation_layer, #nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","          # Dropout layer\n","          nn.Dropout2d(p=0.5),\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l1_channels, out_channels=l2_channels, kernel_size=3, stride=1, padding=1),\n","          # RELU Activation\n","          activation_layer, #nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.linear_layers = nn.Sequential(\n","          # nn.Dropout(p=0.5, inplace=False),\n","          # Linear layer\n","          nn.Linear(in_features=l2_channels*16*16, out_features=1000),   #Flattened image is fed into linear NN and reduced to half size\n","          # RELU Activation\n","          activation_layer, #nn.ReLU(inplace=True),\n","          # Linear final output layer with 16 output features\n","          nn.Linear(in_features=1000, out_features=16)      #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n","        )\n","          \n","    # Forward pass\n","    def forward(self, inputs):\n","      # Getting output from convolution layers\n","      output1 = self.seq_model(inputs)\n","      # Reshaping (flattening) to feed into the subsequent linear layers\n","      output1 = output1.view(output1.size()[0],-1)\n","      # Getting output from final linear layers\n","      outputs = self.linear_layers(output1)\n","      return outputs\n","    \n","class CNN_batch_norm(nn.Module):\n","    def __init__(self):\n","        super(CNN_batch_norm,self).__init__()\n","        l1_channels = 8\n","        l2_channels = 32\n","        # Convolution layers\n","        self.seq_model = nn.Sequential(\n","          # Convolution layer 1 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=1, out_channels=l1_channels, kernel_size=3, stride=1, padding=1),\n","          # Batch normalization\n","          nn.BatchNorm2d(l1_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","          # Dropout layer\n","          nn.Dropout2d(p=0.5),\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l1_channels, out_channels=l2_channels, kernel_size=3, stride=1, padding=1),\n","          # Batch normalization\n","          nn.BatchNorm2d(l2_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.linear_layers = nn.Sequential(\n","          # nn.Dropout(p=0.5, inplace=False),\n","          # Linear layer\n","          nn.Linear(in_features=l2_channels*16*16, out_features=1000),   #Flattened image is fed into linear NN and reduced to half size\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Linear final output layer with 16 output features\n","          nn.Linear(in_features=1000, out_features=16)      #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n","        )\n","          \n","    # Forward pass\n","    def forward(self, inputs):\n","      # Getting output from convolution layers\n","      output1 = self.seq_model(inputs)\n","      # Reshaping (flattening) to feed into the subsequent linear layers\n","      output1 = output1.view(output1.size()[0],-1)\n","      # Getting output from final linear layers\n","      outputs = self.linear_layers(output1)\n","      return outputs\n","\n","class CNN_multi_layer(nn.Module):\n","    def __init__(self):\n","        super(CNN_multi_layer,self).__init__()\n","        l1_channels = 8\n","        l2_channels = 64\n","        l3_channels = 64\n","        l4_channels = 32\n","        # Convolution layers\n","        self.seq_model = nn.Sequential(\n","          # Convolution layer 1 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=1, out_channels=l1_channels, kernel_size=3, stride=1, padding=1),\n","          # Batch normalization\n","          nn.BatchNorm2d(l1_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","          \n","          # # Dropout layer\n","          # nn.Dropout2d(p=0.5),\n","          \n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l1_channels, out_channels=l2_channels, kernel_size=5, stride=1, padding=2),\n","          # Batch normalization\n","          nn.BatchNorm2d(l2_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l2_channels, out_channels=l3_channels, kernel_size=3, stride=1, padding=1),\n","          # Batch normalization\n","          nn.BatchNorm2d(l3_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","          # Dropout layer\n","          nn.Dropout2d(p=0.5),\n","\n","          # Convolution layer 2 with stride 1 and padding 1\n","          nn.Conv2d(in_channels=l3_channels, out_channels=l4_channels, kernel_size=3, stride=1, padding=1),\n","          # Batch normalization\n","          nn.BatchNorm2d(l4_channels),\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Pooling\n","          nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.linear_layers = nn.Sequential(\n","          # nn.Dropout(p=0.5, inplace=False),\n","          # Linear layer\n","          nn.Linear(in_features=l4_channels*4*4, out_features=1000),   #Flattened image is fed into linear NN and reduced to half size\n","          # RELU Activation\n","          nn.ReLU(inplace=True),\n","          # Linear final output layer with 16 output features\n","          nn.Linear(in_features=1000, out_features=16)      #Since there were so many features, I decided to use 45 layers to get output layers. You can increase the kernels in Maxpooling to reduce image further and reduce number of hidden linear layers.\n","        )\n","          \n","    # Forward pass\n","    def forward(self, inputs):\n","      # Getting output from convolution layers\n","      output1 = self.seq_model(inputs)\n","      # Reshaping (flattening) to feed into the subsequent linear layers\n","      output1 = output1.view(output1.size()[0],-1)\n","      # Getting output from final linear layers\n","      outputs = self.linear_layers(output1)\n","      return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNJTDG8xoJwH"},"source":["# ==========================================\n","#         Optimize/Train Network\n","# ==========================================\n","def start_train(model, trainloader_small, optimizer, num_epochs=30, train=True):\n","  debug = True\n","  # Checking if GPU mode available\n","  CUDA = torch.cuda.is_available()\n","  if CUDA:\n","      model = model.cuda()   \n","  \n","  # Using Cross entropy loss function\n","  loss_func = nn.CrossEntropyLoss()   \n","\n","  #Define the lists for storing the results\n","  training_loss = []\n","  training_accuracy = []\n","  start = time.time()\n","  \n","  # For evaluation, we don't need many epochs, its just running once\n","  if train==False:\n","    num_epochs = 1\n","  # Iterating over epochs - training\n","  for epoch in range(num_epochs): \n","      #Resetting the variables at the begining of every epoch\n","      correct_pred = 0\n","      iterations = 0\n","      iteration_loss = 0.0\n","      sample_count = 0\n","      \n","      # Setting the model into train / eval mode based on the type of execution\n","      if train:\n","        model.train()\n","      else:\n","        model.eval()\n","      \n","      # Iterating over all the training batches\n","      for i, (inputs, labels) in enumerate(trainloader_small):\n","          \n","          # Converting to tensor variable (from pytorch>0.4.0, torch ~ variable)\n","          inputs = Variable(inputs)\n","          inputs = inputs.float() \n","          # labels = labels - 1\n","          labels = Variable(labels)\n","          \n","          # If we have GPU, shift the data to GPU\n","          # CUDA = torch.cuda.is_available()\n","          if CUDA:\n","              inputs = inputs.cuda()\n","              labels = labels.cuda()\n","\n","          # Clearing the gradient\n","          optimizer.zero_grad()\n","          # Making a forward pass\n","          outputs = model.forward(inputs)      \n","\n","          # Calculating the loss value\n","          loss_val = loss_func(outputs, labels) \n","          # Accumulating the loss \n","          iteration_loss += loss_val.data\n","          if train:\n","            # Backpropagation\n","            loss_val.backward()\n","            # Updating the weights\n","            optimizer.step()\n","          \n","          # Calculating the correct predictions for training data\n","          _, predicted = torch.max(outputs, 1)\n","          correct_pred += (predicted == labels).sum()\n","          sample_count += len(labels)\n","          iterations += 1\n","\n","          # Clearing to free up some memory\n","          del inputs, labels, outputs, predicted\n","          torch.cuda.empty_cache()\n","\n","      # Storing the training loss\n","      training_loss.append(iteration_loss/iterations)\n","      # Storing the training accuracy\n","      training_accuracy.append((100 * correct_pred / sample_count))\n","      if debug:\n","        if train:\n","          print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n","              .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n","        else:\n","          print ('Epoch {}/{}, Testing Loss: {:.3f}, Testing Accuracy: {:.3f}'\n","              .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n","  stop = time.time()\n","  torch.cuda.empty_cache()\n","  \n","  return training_loss[-1], training_accuracy[-1], (stop-start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKZu2qbiaCto","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1573006998693,"user_tz":300,"elapsed":17428,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"0fec9e90-ecea-4bfc-e252-6946b6efecd6"},"source":["######### Model Training #########\n","# ==========================================\n","#            Evaluating Network\n","# ==========================================\n","\n","######### To analyze the effect of different techniques on the performance of the model\n","epoch_count = 30\n","\n","# Instantiate the model\n","model = CNN_base()\n","# Define the optimizer (SGD is used here)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n","print(\"No augmentation and no zero centering:\")\n","inp_data = trainloader_small_plain\n","# Training\n","loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No augmentation and no zero centering:\n","Training Loss: 0.909, Training Accuracy: 73.000, Time taken: 16.83550\n","Test Loss: 1.957, Test Accuracy: 39.000, Time taken: 0.02299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MiZqOFiAg8i4","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1573006907776,"user_tz":300,"elapsed":34636,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"fdfb89dd-ad93-478b-fb03-d5de08fff9b1"},"source":["# Instantiate the model\n","model = CNN_mod1()\n","# Choose an optimizer (SGD is used here)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.04)\n","print(\"\\nBoth augmentation and zero centering, and dropout of 0.5 :\")\n","inp_data = trainloader_small\n","# Training\n","loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Both augmentation and zero centering, and dropout of 0.5 :\n","Training Loss: 0.062, Training Accuracy: 98.000, Time taken: 34.00116\n","Test Loss: 2.526, Test Accuracy: 46.000, Time taken: 0.02294\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jq8bR2xUXe-4","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1573008360810,"user_tz":300,"elapsed":12567,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"288b3c57-a14e-4a02-863d-bc64e45abb53"},"source":["# Data augmentation using mirroring, random rotation, random noise addition\n","trainloader_complete_aug = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=True, zero_centered=True, aug_list=[True,True,True]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","9600\n","50\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IqiaGuU7mTpb","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1573008452113,"user_tz":300,"elapsed":91282,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"b1b499b5-1441-452f-96a1-2f6a89ae6c71"},"source":["# Instantiate the model\n","model = CNN_mod1()\n","# Choose an optimizer (SGD is used here)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.04)\n","print(\"Data augmentation using mirroring, random rotation, random noise addition:\")\n","inp_data = trainloader_complete_aug\n","# Training\n","loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data augmentation using mirroring, random rotation, random noise addition:\n","Training Loss: 0.033, Training Accuracy: 98.000, Time taken: 90.53341\n","Test Loss: 2.647, Test Accuracy: 48.000, Time taken: 0.02220\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nHz5eB25Yszy","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1573006296478,"user_tz":300,"elapsed":37746,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"ed46c4e4-42d0-4341-94cb-a6cf9f34d4ec"},"source":["# Batch normalization\n","# Instantiate the model\n","model = CNN_batch_norm()\n","# Choose an optimizer (SGD is used here)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\n","print(\"Batch normalization:\")\n","inp_data = trainloader_small\n","# Training\n","loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Batch normalization:\n","Training Loss: 0.066, Training Accuracy: 98.000, Time taken: 37.13164\n","Test Loss: 1.694, Test Accuracy: 51.000, Time taken: 0.02433\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OYEj-15sYo37","colab":{"base_uri":"https://localhost:8080/","height":238},"executionInfo":{"status":"ok","timestamp":1573006140696,"user_tz":300,"elapsed":153513,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"87e9849a-76aa-4faa-ea07-8f4d8fa780c2"},"source":["print(\"Different activation layers:\")\n","for key, value in {\"ReLU\": nn.ReLU(inplace=True), \"Sigmoid\": nn.Sigmoid(), \"Tanh\": nn.Tanh()}.items():\n","  # Instantiate the model\n","  model = CNN_act_fn(nn.ReLU(inplace=True))\n","  # Choose an optimizer (SGD is used here)\n","  optimizer = torch.optim.SGD(model.parameters(), lr = 0.05)\n","  print(\"\\n\" + key + \" activation layer:\")\n","  inp_data = trainloader_small\n","  # Training\n","  loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","  print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","                .format(loss, accuracy, time_taken))\n","  # Prediction\n","  loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","  print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","                .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Different activation layers:\n","\n","ReLU activation layer:\n","Training Loss: 0.058, Training Accuracy: 98.000, Time taken: 33.87798\n","Test Loss: 2.152, Test Accuracy: 51.000, Time taken: 0.02311\n","\n","Sigmoid activation layer:\n","Training Loss: 0.071, Training Accuracy: 98.000, Time taken: 33.95397\n","Test Loss: 2.358, Test Accuracy: 43.000, Time taken: 0.02288\n","\n","Tanh activation layer:\n","Training Loss: 0.071, Training Accuracy: 97.000, Time taken: 33.96816\n","Test Loss: 2.716, Test Accuracy: 44.000, Time taken: 0.02299\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7csijEHGhnF_","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1573008079843,"user_tz":300,"elapsed":68334,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"74fb472e-3aec-46f7-baed-832647118e5b"},"source":["# Modifying architecture\n","# Instantiate the model\n","model = CNN_multi_layer()\n","# Choose an optimizer (SGD is used here)\n","optimizer = torch.optim.SGD(model.parameters(), lr = 0.04)\n","print(\"Changing the arhitecture of the model:\")\n","inp_data = trainloader_small\n","epoch_count = 40\n","# Training\n","loss, accuracy, time_taken = start_train(model, inp_data, optimizer, num_epochs=epoch_count)\n","print ('Training Loss: {:.3f}, Training Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(model, testloader_small, optimizer, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Changing the arhitecture of the model:\n","Training Loss: 0.085, Training Accuracy: 97.000, Time taken: 67.48110\n","Test Loss: 1.458, Test Accuracy: 60.000, Time taken: 0.03407\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"nBHKIxzAYYM2"},"source":["## Fine Tuning a Pre-Trained Deep Network:\n","Fine tuning Alexnet and performing classification task"]},{"cell_type":"code","metadata":{"id":"VLG3WtEmYYM3","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1573032834194,"user_tz":300,"elapsed":47547,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"7d584020-2a22-4e70-cbd8-b7c52d8de9b1"},"source":["# reload data with a larger size\n","img_size = (224, 224)\n","batch_num = 50 # training sample number per batch \n","\n","# load training dataset\n","trainloader_large = list(load_dataset('./data/train/', img_size, batch_num=batch_num, shuffle=True, \n","                                      augment=False, is_color=True, zero_centered=True))\n","train_num = len(trainloader_large)\n","print(\"Finish loading %d minibatches(=%d) of training samples.\" % (train_num, batch_num))\n","\n","# load testing dataset\n","testloader_large = list(load_dataset('./data/test/', img_size, num_per_class=50, batch_num=batch_num, is_color=True))\n","test_num = len(testloader_large)\n","print(\"Finish loading %d minibatches(=%d) of testing samples.\" % (test_num, batch_num))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","2400\n","50\n","Finish loading 48 minibatches(=50) of training samples.\n","Loading images from class: 0\n","Loading images from class: 1\n","Loading images from class: 2\n","Loading images from class: 3\n","Loading images from class: 4\n","Loading images from class: 5\n","Loading images from class: 6\n","Loading images from class: 7\n","Loading images from class: 8\n","Loading images from class: 9\n","Loading images from class: 10\n","Loading images from class: 11\n","Loading images from class: 12\n","Loading images from class: 13\n","Loading images from class: 14\n","Loading images from class: 15\n","Loading images from class: 16\n","400\n","50\n","Finish loading 8 minibatches(=50) of testing samples.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"btOal_ampEnm","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1572913995465,"user_tz":300,"elapsed":305415,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"b6f7fb96-0508-41c0-8c6c-ed37cbc7e44b"},"source":["# ==========================================\n","#       Fine-Tune Pretrained Network\n","# ==========================================\n","import torchvision.models as models\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# Instantiate the alexnet model\n","alexnet_model = models.alexnet(pretrained=True)   # Replacing two layers\n","alexnet_model_l0 = models.alexnet(pretrained=True) # Replacing one layer\n","\n","# Getting the feature count in the last 2 layers\n","l1_num_ftrs = alexnet_model.classifier._modules['4'].in_features\n","l1_out_num_ftrs = 3000\n","l0_num_ftrs = alexnet_model.classifier._modules['6'].in_features\n","\n","# Replacing the second linear layer from the last layer with custom layer\n","alexnet_model.classifier._modules['4'] = nn.Linear(l1_num_ftrs, l1_out_num_ftrs)\n","\n","# Replacing the last liner layer with custom layer\n","alexnet_model.classifier._modules['6'] = nn.Linear(l1_out_num_ftrs, 16)\n","alexnet_model = alexnet_model.to(device)\n","\n","# Replacing the last liner layer with custom layer (in a different model)\n","alexnet_model_l0.classifier._modules['6'] = nn.Linear(l0_num_ftrs, 16)\n","alexnet_model_l0 = alexnet_model_l0.to(device)\n","\n","print(\"Replacing last two layers:\")\n","# Defining the optimizer function\n","optimizer_ft = torch.optim.SGD(alexnet_model.parameters(), lr=0.001, momentum=0.9)\n","# print(alexnet_model)\n","# Training\n","loss, accuracy, time_taken = start_train(alexnet_model, trainloader_large, optimizer_ft, num_epochs=30)\n","print ('Train Loss: {:.3f}, Train Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","# Prediction\n","loss, accuracy, time_taken = start_train(alexnet_model, testloader_large, optimizer_ft, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","\n","print(\"Replacing last one layer:\")\n","optimizer_ft_l0 = torch.optim.SGD(alexnet_model_l0.parameters(), lr=0.001, momentum=0.9)\n","# print(alexnet_model_l0)\n","loss, accuracy, time_taken = start_train(alexnet_model_l0, trainloader_large, optimizer_ft_l0, num_epochs=30)\n","print ('Train Loss: {:.3f}, Train Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))\n","loss, accuracy, time_taken = start_train(alexnet_model_l0, testloader_large, optimizer_ft_l0, train = False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'\n","              .format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Replacing last two layers:\n","Train Loss: 0.012, Train Accuracy: 99.000, Time taken: 149.05260\n","Test Loss: 1.018, Test Accuracy: 78.000, Time taken: 0.27759\n","Replacing last one layer:\n","Train Loss: 0.008, Train Accuracy: 99.000, Time taken: 152.50649\n","Test Loss: 0.812, Test Accuracy: 84.000, Time taken: 0.28323\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JhAaQguTozX4"},"source":["Alexnet + SVM classifier:"]},{"cell_type":"code","metadata":{"id":"wsapqVoux4Lc","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1573027857514,"user_tz":300,"elapsed":5990,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"2337bb26-0537-44e5-e0fe-2d9a3f181e74"},"source":["import torchvision.models as models\n","from sklearn import svm\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","# Instantiating the model\n","alexnet_model_svm = models.alexnet(pretrained=True)\n","# Selecting the optimizer function (SGD here)\n","optimizer_ft = torch.optim.SGD(alexnet_model_svm.parameters(), lr=0.001, momentum=0.9)\n","# Removing the last layer i.e., reading the features from last but one layer\n","new_classifier = list(alexnet_model_svm.classifier.children())[:-2]\n","# Updating the classifier model\n","alexnet_model_svm.classifier =  nn.Sequential(*new_classifier)\n","alexnet_model_svm = alexnet_model_svm.to(device)\n","\n","# Function to get features from Alexnet\n","def get_alexnet_features(alexnet_model_svm, trainloader_large):\n","  train_X = []\n","  train_Y = np.array([])\n","  for i, (inputs, labels) in enumerate(trainloader_large):  \n","    # Convert to torch Variable\n","    inputs = Variable(inputs)\n","    inputs = inputs.float() \n","    labels = Variable(labels)\n","    \n","    # Check if GPU can be used\n","    CUDA = torch.cuda.is_available()\n","    if CUDA:\n","        inputs = inputs.cuda()\n","        labels = labels.cuda()\n","    # Make the forward pass\n","    outputs = alexnet_model_svm.forward(inputs)[:,:300]  \n","    # Accumulate the features (output from Alexnet model is input to SVM)\n","    train_X = train_X + list(outputs.cpu().data.numpy())\n","    train_Y = np.concatenate((train_Y, labels.cpu().data.numpy()), axis=0)\n","\n","  train_X = np.array(train_X)\n","  # Return the features\n","  return train_X, train_Y\n","\n","start = time.time()\n","# Get the train feature data for SVM classifier\n","train_X, train_Y = get_alexnet_features(alexnet_model_svm, trainloader_large)\n","# Get the test feature data\n","test_X, test_Y = get_alexnet_features(alexnet_model_svm, testloader_large)\n","\n","\n","# Instantiate the SVM classifier\n","model_svc = svm.LinearSVC(C=0.00103, max_iter=5000)\n","# Train the SVM classifier\n","accuracy = model_svc.fit(train_X, train_Y).score(train_X, train_Y) * 100\n","time_taken = time.time()-start\n","print ('Train Accuracy: {:.3f}, Time taken: {:.5f}'.format(accuracy, time_taken))\n","\n","start = time.time()\n","# Make predictions\n","pred_labels = model_svc.predict(test_X)\n","\n","time_taken = time.time()-start\n","# Find the count of correct labels\n","correct = (pred_labels == test_Y).sum()\n","\n","# Calculate the accuracy\n","accuracy = float(correct) / len(test_Y) * 100\n","print ('Test Accuracy: {:.3f}, Time taken: {:.5f}'.format(accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train Accuracy: 97.250, Time taken: 4.30311\n","Test Accuracy: 75.750, Time taken: 0.00140\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"86aX6JWFztG9"},"source":["VGG + SVM classifier "]},{"cell_type":"code","metadata":{"id":"ar0zL3OtxjKB"},"source":["def start_test_vgg(model, trainloader_small, optimizer, num_epochs=30, train=True):\n","  debug = False \n","  \n","  # Using Cross entropy loss function\n","  loss_func = nn.CrossEntropyLoss()   \n","\n","  # Define the lists for storing the results\n","  training_loss = []\n","  training_accuracy = []\n","  start = time.time()\n","  \n","  # For evaluation, we don't need many epochs, its just running once\n","  if train==False:\n","    num_epochs = 1\n","  # Iterating over epochs - training\n","  for epoch in range(num_epochs): \n","      #Resetting the variables at the begining of every epoch\n","      correct_pred = 0\n","      iterations = 0\n","      iteration_loss = 0.0\n","      sample_count = 0\n","      \n","      # Setting the model into train / eval mode based on the type of execution\n","      if train:\n","        model.train()\n","      else:\n","        model.eval()\n","      with torch.no_grad():\n","        # Iterating over all the training batches\n","        for (inputs, labels) in trainloader_small[0:1]:\n","            \n","            # Converting to tensor variable (from pytorch>0.4.0, torch ~ variable)\n","            inputs = Variable(inputs)\n","            inputs = inputs.float() \n","            # labels = labels - 1\n","            labels = Variable(labels)\n","            \n","            # If we have GPU, shift the data to GPU\n","            CUDA = torch.cuda.is_available()\n","            if CUDA:\n","                inputs = inputs.cuda()\n","                labels = labels.cuda()\n","\n","            # Clearing the gradient\n","            optimizer.zero_grad()\n","            # Making a forward pass\n","            outputs = model.forward(inputs)      \n","\n","            # Calculating the loss value\n","            loss_val = loss_func(outputs, labels) \n","            # Accumulating the loss \n","            iteration_loss += loss_val.data\n","            if train:\n","              # Backpropagation\n","              loss_val.backward()\n","              # Updating the weights\n","              optimizer.step()\n","            \n","            # Calculating the correct predictions for training data\n","            _, predicted = torch.max(outputs, 1)\n","            correct_pred += (predicted == labels).sum()\n","            sample_count += len(labels)\n","            iterations += 1\n","\n","            # Clearing to free up some memory\n","            del inputs, labels, outputs, predicted\n","            torch.cuda.empty_cache()\n","\n","      # Storing the training loss\n","      training_loss.append(iteration_loss/iterations)\n","      # Storing the training accuracy\n","      training_accuracy.append((100 * correct_pred / sample_count))\n","      if debug:\n","        if train:\n","          print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n","              .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n","        else:\n","          print ('Epoch {}/{}, Testing Loss: {:.3f}, Testing Accuracy: {:.3f}'\n","              .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n","  stop = time.time()\n","  torch.cuda.empty_cache()\n","  \n","  return training_loss[-1], training_accuracy[-1], (stop-start)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"miz77ra-Bu6d"},"source":["######### VGG #########\n","\n","import torchvision.models as models\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model_vgg16 = models.vgg16_bn(pretrained=True)\n","\n","# print(model_vgg16)\n","\n","# Getting the feature count in the last 2 layers\n","l0_num_ftrs = model_vgg16.classifier._modules['6'].in_features\n","\n","# Replacing the last liner layer with custom layer\n","model_vgg16.classifier._modules['6'] = nn.Linear(l0_num_ftrs, 16)\n","model_vgg16 = model_vgg16.to(device)\n","\n","optimizer_func = torch.optim.SGD(model_vgg16.parameters(), lr=0.01, momentum=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7KGunqpArUd","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1573029227841,"user_tz":300,"elapsed":1137181,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"f978d4b1-cd02-462e-8622-c0426b9d9604"},"source":["# Train Vgg\n","\n","loss, accuracy, time_taken = start_train(model_vgg16, trainloader_large, optimizer_func, num_epochs=10)\n","print ('Train Loss: {:.3f}, Train Accuracy: {:.3f}, Time taken: {:.5f}'.format(loss, accuracy, time_taken))\n","\n","# Saving the model for easy testing\n","# torch.save(model_vgg16.state_dict(), save_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 2/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 3/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 4/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 5/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 6/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 7/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 8/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 9/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Epoch 10/10, Training Loss: 0.000, Training Accuracy: 100.000\n","Train Loss: 0.000, Train Accuracy: 100.000, Time taken: 1136.56317\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MP8QuA8jAf5j","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1573032876731,"user_tz":300,"elapsed":1378,"user":{"displayName":"Halady Akhilesh Miththanthaya","photoUrl":"","userId":"14170158901396127168"}},"outputId":"2ed52a65-d21b-464b-f02f-562fd8de2cda"},"source":["# Test Vgg\n","\n","# model_vgg16.load_state_dict(torch.load(save_path))\n","loss, accuracy, time_taken = start_test_vgg(model_vgg16, testloader_large, optimizer_func, train=False)\n","print ('Test Loss: {:.3f}, Test Accuracy: {:.3f}, Time taken: {:.5f}'.format(loss, accuracy, time_taken))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test Loss: 0.084, Test Accuracy: 96.000, Time taken: 0.89257\n"],"name":"stdout"}]}]}