{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "name": "Copy of CSE527_HW5_fall19.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcamCuuyjYb2"
      },
      "source": [
        "# Action Recognition over UCF101  \n",
        "\n",
        "We use a pre-trained model (VGG16) to extract features from each frame (output from VGG 4096x25 for the first 25 frames of every video) and it is fed to an LSTM network which takes a **dx25** sample as input (where **d** is the dimension of the extracted feature for each frame), and outputs the action label of that sample.\n",
        "\n",
        "\n",
        "Compare the performance with a SVM trained over stacked **dx25** feature matrix.\n",
        "\n",
        "\n",
        "Raw images of 256x340 are resized by cropping five **nxn** images, one at the image center and four at the corners and compute the **d**-dim features for each of them, and average these five **d**-dim feature to get a final feature representation for the raw image.\n",
        "\n",
        "The first 25 classes of the whole dataset are initially considered.\n",
        "\n",
        "## Dataset\n",
        "Download dataset at [UCF101](http://vision.cs.stonybrook.edu/~yangwang/public/UCF101_images.tar)(Image data for each video) \n",
        "\n",
        "**annos** folder has the video labels and the label to class name mapping."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCRwkL7jxtc"
      },
      "source": [
        "---\n",
        "Feature extraction:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeLBRievfYMy"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from skimage import io\n",
        "from sklearn import svm\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urKQi8oAjYb-"
      },
      "source": [
        "# Initialize variables\n",
        "low_class_count_bound =1\n",
        "class_count = 25\n",
        "video_labels_file = \"annos/videos_labels_subsets.txt\"\n",
        "actions_file = \"annos/actions.txt\"\n",
        "\n",
        "# Read actions and labels file\n",
        "actions_map = pd.read_csv(actions_file, sep=\"  \", header=None, engine='python')\n",
        "actions_map.columns = [\"label\", \"label_name\"]\n",
        "\n",
        "label_map = pd.read_csv(video_labels_file, sep=\"\\t\", header=None)\n",
        "label_map.columns = [\"image_name\", \"label\", \"train_flag\"]\n",
        "\n",
        "# Filter based on the label\n",
        "filtered_label_map = label_map.loc[label_map.label <=class_count].loc[label_map.label >=low_class_count_bound]\n",
        "filtered_label_map = filtered_label_map.set_index('label').join(actions_map.set_index('label'), lsuffix='_caller', rsuffix='_other').reset_index()\n",
        "train_label_map = filtered_label_map.loc[filtered_label_map.train_flag == 1]\n",
        "test_label_map = filtered_label_map.loc[filtered_label_map.train_flag == 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnc42A-6fYM2"
      },
      "source": [
        "# Dataset class\n",
        "class VideoImageDataset(Dataset):\n",
        "    def __init__(self, init_df, root_dir, transform=None, crop_transform=None):\n",
        "        # initialization\n",
        "        self.labels_frame = init_df\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.crop_transform = crop_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_frame)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        # Iterating over all the images in folder\n",
        "        img_list = []\n",
        "        img_folder_path = os.path.join(self.root_dir, self.labels_frame.iloc[idx, 1])\n",
        "        img_files = [os.path.join(img_folder_path, img) for img in os.listdir(img_folder_path) if os.path.isfile(os.path.join(img_folder_path, img))]\n",
        "        for img_file in img_files:\n",
        "            # reading the image\n",
        "            img_list.append(io.imread(img_file))\n",
        "\n",
        "        \n",
        "        # Applying transformations like normalizing, 5-cropping\n",
        "        if self.transform:\n",
        "            img_norm_list = []\n",
        "            for i in range(len(img_list)):\n",
        "                # normalize\n",
        "                img_norm_list.append(self.transform(img_list[i]))\n",
        "            img_tuple = {'img_list': img_norm_list, 'labels': self.labels_frame.iloc[idx,0]}\n",
        "            # 5-crop\n",
        "            img_tuple = self.crop_transform(img_tuple)\n",
        "        else:\n",
        "            img_tuple = {'img_list': img_list, 'labels': self.labels_frame.iloc[idx,0]}\n",
        "        return img_tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHyT87NIfYM4"
      },
      "source": [
        "######### VGG #########\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# pretrained vgg16 for feature extraction\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_vgg16 = models.vgg16_bn(pretrained=True)\n",
        "\n",
        "# updating the classifier so as to extract features from the first fully connected layer\n",
        "new_classifier = list(model_vgg16.classifier.children())[0:2]\n",
        "# Updating the classifier model\n",
        "model_vgg16.classifier =  nn.Sequential(*new_classifier)\n",
        "model_vgg16.eval()\n",
        "model_vgg16 = model_vgg16.to(device)\n",
        "output_size = (224,224)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weL71YpdfYM6"
      },
      "source": [
        "# 5-Crop transformation class\n",
        "class Crop(object):\n",
        "    def __init__(self, output_size, vgg_model):\n",
        "        # initialization\n",
        "        self.output_size = output_size\n",
        "        self.vgg_model = vgg_model\n",
        "\n",
        "    # Function to extract features using vgg16\n",
        "    def get_vgg_features(self, img):\n",
        "        with torch.no_grad():\n",
        "            # Convert to torch Variable\n",
        "            img = torch.from_numpy(img)\n",
        "            img = img.float() \n",
        "\n",
        "            # Check if GPU can be used\n",
        "            CUDA = torch.cuda.is_available()\n",
        "            if CUDA:\n",
        "                img = img.cuda()\n",
        "            # Make the forward pass\n",
        "            outputs = self.vgg_model.forward(img)\n",
        "            # Clearing to free up some memory\n",
        "            del img\n",
        "        \n",
        "        torch.cuda.empty_cache()\n",
        "        # Return the features\n",
        "        return outputs\n",
        "        \n",
        "    def __call__(self, img_tuple):\n",
        "        n = output_size[0]\n",
        "        img_array = np.empty((0, 3, 224, 224))\n",
        "        img_list, labels = img_tuple['img_list'], img_tuple['labels']\n",
        "        img_feat_list = []\n",
        "        # 5-cropping - 4 corners and one at the centre\n",
        "        for i in range(len(img_list)):\n",
        "            c,x,y = img_list[i].shape\n",
        "            img_array = np.append(img_array, np.transpose(np.expand_dims(img_list[i][:,:n,:n], axis=0), axes=[0,1,2,3]), axis=0)\n",
        "            img_array = np.append(img_array, np.transpose(np.expand_dims(img_list[i][:,-n:,:n], axis=0), axes=[0,1,2,3]), axis=0)\n",
        "            img_array = np.append(img_array, np.transpose(np.expand_dims(img_list[i][:,:n,-n:], axis=0), axes=[0,1,2,3]), axis=0)\n",
        "            img_array = np.append(img_array, np.transpose(np.expand_dims(img_list[i][:,-n:,-n:], axis=0), axes=[0,1,2,3]), axis=0)\n",
        "            img_array = np.append(img_array, np.transpose(np.expand_dims(img_list[i][:,x//2-n//2:x//2+n//2,y//2-n//2:y//2+n//2], axis=0), \n",
        "                                                          axes=[0,1,2,3]), axis=0)\n",
        "        # get the image features from vgg model\n",
        "        img_feat_array = self.get_vgg_features(img_array)\n",
        "        \n",
        "        for i in range(len(img_list)):\n",
        "            # take the mean of 5-crop images of the main image\n",
        "            img_feat_list.append(torch.mean(img_feat_array[i:i+5,:], 0,False))\n",
        "       \n",
        "        return {'img_list': img_feat_list, 'labels':labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uEcO0SIfYM7"
      },
      "source": [
        "# generate train dataset\n",
        "\n",
        "# normalize transform\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# training dataset\n",
        "train_video_dataset = VideoImageDataset(train_label_map, root_dir=r\"data/images\", \n",
        "                                  transform=transforms.Compose([\n",
        "                                               transforms.ToTensor(), normalize]),\n",
        "                                  crop_transform=transforms.Compose([Crop(output_size, model_vgg16)]))\n",
        "print(\"Total count: %d\"%len(train_video_dataset))\n",
        "\n",
        "# Saving the dataset for reuse\n",
        "dataset_list = []\n",
        "start = time.time()\n",
        "for i in range(len(train_video_dataset)):\n",
        "    print(i)\n",
        "    dataset_list.append(train_video_dataset[i])\n",
        "print(\"Time taken for feature generation: %f\"%(time.time()-start))\n",
        "start = time.time()\n",
        "torch.save(dataset_list, \"train_dataset_\"+str(low_class_count_bound)+\"_to_\"+str(class_count)+\".pt\")\n",
        "print(\"Time taken for saving features: %f\"%(time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IBQGp7kfYM9",
        "outputId": "72346e59-ca01-4c5d-86e5-4b386326236c"
      },
      "source": [
        "# Load the train dataset\n",
        "\n",
        "start = time.time()\n",
        "train_video_dataset = torch.load(\"train_dataset_\"+str(low_class_count_bound)+\"_to_\"+str(class_count)+\".pt\")\n",
        "print(\"Time taken for loading features: %f\"%(time.time()-start))\n",
        "print(\"Total count: %d\"%len(train_video_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for loading features: 5.327873\n",
            "Total count: 2409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBhAvC6ZfYM_"
      },
      "source": [
        "# generate test dataset\n",
        "\n",
        "# normalize transform\n",
        "test_video_dataset = VideoImageDataset(test_label_map, root_dir=r\"data/images\", \n",
        "                                  transform=transforms.Compose([\n",
        "                                               transforms.ToTensor(), normalize]),\n",
        "                                  crop_transform=transforms.Compose([Crop(output_size, model_vgg16)]))\n",
        "print(\"Total count: %d\"%len(test_video_dataset))\n",
        "      \n",
        "# Saving the dataset for reuse\n",
        "dataset_list = []\n",
        "start = time.time()\n",
        "for i in range(len(test_video_dataset)):\n",
        "    print(i)\n",
        "    dataset_list.append(test_video_dataset[i])\n",
        "print(\"Time taken for feature generation: %f\"%(time.time()-start))\n",
        "start = time.time()\n",
        "torch.save(dataset_list, \"test_dataset_\"+str(low_class_count_bound)+\"_to_\"+str(class_count)+\".pt\")\n",
        "print(\"Time taken for saving features: %f\"%(time.time()-start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vx0LM1nxfYNB",
        "outputId": "f20b1fcb-774e-4f80-a49c-9a396c187415"
      },
      "source": [
        "# Load the test dataset\n",
        "\n",
        "start = time.time()\n",
        "test_video_dataset = torch.load(\"test_dataset_\"+str(low_class_count_bound)+\"_to_\"+str(class_count)+\".pt\")\n",
        "print(\"Time taken for loading features: %f\"%(time.time()-start))\n",
        "print(\"Total count: %d\"%len(test_video_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for loading features: 2.217070\n",
            "Total count: 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oUKTg9lfYND"
      },
      "source": [
        "# reshaping data for feeding to the network\n",
        "\n",
        "for i in range(len(train_video_dataset)):\n",
        "    for j in range(len(train_video_dataset[i]['img_list'])):\n",
        "        train_video_dataset[i]['img_list'][j] = train_video_dataset[i]['img_list'][j].reshape(-1).cpu().numpy()\n",
        "    #train_video_dataset[i]['img_list'] = np.transpose(np.array(train_video_dataset[i]['img_list']), axes=[1,0])\n",
        "    train_video_dataset[i]['img_list'] = np.array(train_video_dataset[i]['img_list'])\n",
        "for i in range(len(test_video_dataset)):\n",
        "    for j in range(len(test_video_dataset[i]['img_list'])):\n",
        "        test_video_dataset[i]['img_list'][j] = test_video_dataset[i]['img_list'][j].reshape(-1).cpu().numpy()\n",
        "#     test_video_dataset[i]['img_list'] = np.transpose(np.array(test_video_dataset[i]['img_list']), axes=[1,0])\n",
        "    test_video_dataset[i]['img_list'] = np.array(test_video_dataset[i]['img_list'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN74WLWpl7zQ"
      },
      "source": [
        "***\n",
        "Modelling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EGU31IJn5_h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b3541df6-f994-4f0f-ffde-d63430381e01"
      },
      "source": [
        "print('Shape of training data is :', train_video_dataset[0]['img_list'].shape)\n",
        "print('Number of training records is :', len(train_video_dataset))\n",
        "print('Shape of test/validation data is :', test_video_dataset[0]['img_list'].shape)\n",
        "print('Number of testing records is :', len(test_video_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training data is : (25, 4096)\n",
            "Number of training records is : 2409\n",
            "Shape of test/validation data is : (25, 4096)\n",
            "Number of testing records is : 951\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqshyjO3mHkt"
      },
      "source": [
        "# LSTM classifier model class\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=4096, num_lstm_layers=2, hidden_layer_size=[200], \n",
        "                 output_size=class_count, dropout=0, bidirectional=False):\n",
        "        \n",
        "        super().__init__()\n",
        "        # initialization\n",
        "        self.hidden_layer_size = hidden_layer_size\n",
        "        self.num_lstm_layers = num_lstm_layers\n",
        "        if bidirectional:\n",
        "            self.directions = 2\n",
        "        else:\n",
        "            self.directions = 1\n",
        "        # hidden state and cell state\n",
        "        self.hidden_cell = \\\n",
        "            (torch.zeros(self.num_lstm_layers*self.directions, 10, \n",
        "                self.hidden_layer_size[0]).cuda(), \n",
        "             torch.zeros(self.num_lstm_layers*self.directions, 10, \n",
        "                self.hidden_layer_size[0]).cuda())\n",
        "        \n",
        "        # LSTM model\n",
        "        self.lstm = nn.LSTM(input_size, hidden_layer_size[0], \n",
        "                            num_layers=num_lstm_layers, batch_first=True, \n",
        "                            dropout=dropout, bidirectional=bidirectional)\n",
        "        \n",
        "        # Sequential model (generating dynamically based on the input hidden dimensions list)\n",
        "        sequential_model_list = []\n",
        "        \n",
        "        # if only one hidden layer\n",
        "        # first layer\n",
        "        if len(hidden_layer_size) == 1:\n",
        "            sequential_model_list.append(nn.Linear(in_features=hidden_layer_size[0]*self.directions, \n",
        "                    out_features=output_size))\n",
        "        \n",
        "        # if more than one hidden layers\n",
        "        else:\n",
        "            for i in range(len(hidden_layer_size)-1):\n",
        "                # first layer\n",
        "                if i == 0:\n",
        "                    sequential_model_list.append(nn.Linear(in_features=hidden_layer_size[i]*self.directions, \n",
        "                        out_features=hidden_layer_size[i+1]))\n",
        "                # subsequent layers\n",
        "                else:\n",
        "                    # activation function\n",
        "                    sequential_model_list.append(nn.ReLU(inplace=True))\n",
        "                    # dropout layer\n",
        "                    sequential_model_list.append(nn.Dropout())\n",
        "                    # hidden layer\n",
        "                    sequential_model_list.append(nn.Linear(in_features=hidden_layer_size[i], \n",
        "                        out_features=hidden_layer_size[i+1]))\n",
        "            sequential_model_list.append(nn.ReLU(inplace=True))\n",
        "            # last layer\n",
        "            sequential_model_list.append(nn.Linear(in_features=hidden_layer_size[-1], \n",
        "                        out_features=output_size))\n",
        "        # creating a sequential model\n",
        "        self.linear_layers = nn.Sequential(*sequential_model_list)\n",
        "    \n",
        "    # forward pass\n",
        "    def forward(self, input_seq):\n",
        "        # pass through LSTM\n",
        "        lstm_out,self.hidden_cell = self.lstm(input_seq, self.hidden_cell)\n",
        "        # pass through hidden layers\n",
        "        predictions = self.linear_layers(lstm_out[:,-1,:])\n",
        "        # applying softmax to get label scores\n",
        "        label_scores = F.log_softmax(predictions, dim=1)\n",
        "        return label_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZGwNYsfYNJ"
      },
      "source": [
        "# train / predict function\n",
        "\n",
        "def start_train(model, dataloader, optimizer, num_epochs=30, train=True):\n",
        "    debug = False\n",
        "    # Checking if GPU mode available\n",
        "    CUDA = torch.cuda.is_available()\n",
        "    if CUDA:\n",
        "        model = model.cuda()\n",
        "\n",
        "    # Using Cross entropy loss function\n",
        "    loss_func = nn.CrossEntropyLoss()   \n",
        "\n",
        "    #Define the lists for storing the results\n",
        "    training_loss = []\n",
        "    training_accuracy = []\n",
        "    start = time.time()\n",
        "\n",
        "    # For evaluation, we don't need many epochs, its just running once\n",
        "    if train==False:\n",
        "        num_epochs = 1\n",
        "    # Iterating over epochs - training\n",
        "    for epoch in range(num_epochs): \n",
        "        #Resetting the variables at the begining of every epoch\n",
        "        correct_pred = 0\n",
        "        iterations = 0\n",
        "        iteration_loss = 0.0\n",
        "        sample_count = 0\n",
        "\n",
        "        # Setting the model into train / eval mode based on the type of execution\n",
        "        if train:\n",
        "            model.train()\n",
        "        else:\n",
        "            model.eval()\n",
        "\n",
        "        # Iterating over all the training batches\n",
        "        for i, data_batch in enumerate(dataloader):\n",
        "            inputs = Variable(data_batch['img_list'])\n",
        "            batch_size = len(inputs)\n",
        "            \n",
        "            # hidden and cell states of LSTM model\n",
        "            lstm_model.hidden_cell = \\\n",
        "            (torch.zeros(model.num_lstm_layers*model.directions, batch_size, \n",
        "                lstm_model.hidden_layer_size[0]).cuda(), \n",
        "             torch.zeros(model.num_lstm_layers*model.directions, batch_size, \n",
        "                lstm_model.hidden_layer_size[0]).cuda())\n",
        "            \n",
        "            # Converting to tensor variable (from pytorch>0.4.0, torch ~ variable)\n",
        "            labels = Variable(data_batch['labels'])\n",
        "            inputs = Variable(inputs)\n",
        "            inputs = inputs.float() \n",
        "            labels = labels - 1\n",
        "            labels = Variable(labels)\n",
        "\n",
        "            # If we have GPU, shift the data to GPU\n",
        "            if CUDA:\n",
        "                inputs = inputs.cuda()\n",
        "                labels = labels.cuda()\n",
        "\n",
        "            # Clearing the gradient\n",
        "            optimizer.zero_grad()\n",
        "            # Making a forward pass\n",
        "            outputs = model.forward(inputs)      \n",
        "\n",
        "            # Calculating the loss value\n",
        "            loss_val = loss_func(outputs, labels) \n",
        "            # Accumulating the loss \n",
        "            iteration_loss += loss_val.data\n",
        "            if train:\n",
        "                # Backpropagation\n",
        "                loss_val.backward()\n",
        "                # Updating the weights\n",
        "                optimizer.step()\n",
        "\n",
        "            # Calculating the correct predictions for training data\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_pred += (predicted == labels).sum()\n",
        "            sample_count += len(labels)\n",
        "            iterations += 1\n",
        "\n",
        "            # Clearing to free up some memory\n",
        "            del inputs, labels, outputs, predicted\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # Storing the training loss\n",
        "        training_loss.append(iteration_loss/iterations)\n",
        "        # Storing the training accuracy\n",
        "        training_accuracy.append((100.0 * correct_pred / sample_count))\n",
        "        if debug:\n",
        "            if train:\n",
        "                print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}'\n",
        "                  .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n",
        "            else:\n",
        "                print ('Epoch {}/{}, Testing Loss: {:.3f}, Testing Accuracy: {:.3f}'\n",
        "                  .format(epoch+1, num_epochs, training_loss[-1], training_accuracy[-1]))\n",
        "    stop = time.time()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return training_loss[-1], training_accuracy[-1], (stop-start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJnfX6NGfYNK"
      },
      "source": [
        "# create the model\n",
        "\n",
        "lstm_model = LSTM(num_lstm_layers=1, hidden_layer_size=[1000,100], \n",
        "                  dropout=0, bidirectional=True)\n",
        "# loss_function = nn.MSELoss()\n",
        "# loss_function = nn.NLLLoss()\n",
        "\n",
        "# optimizer to be used\n",
        "\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "optimizer = torch.optim.SGD(lstm_model.parameters(), lr=0.05)\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# lstm_model = lstm_model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0otrsXBBfYNM",
        "outputId": "e48e78d1-bd5f-4559-d57b-1ed828d909b0"
      },
      "source": [
        "# train the model\n",
        "\n",
        "batch_size = 10\n",
        "num_epochs = 30\n",
        "print(lstm_model)\n",
        "\n",
        "# load train dataset\n",
        "dataloader = DataLoader(train_video_dataset, batch_size=batch_size, \n",
        "                        shuffle=True, num_workers=0)\n",
        "train_loss, train_accuracy, train_time = start_train(lstm_model, dataloader, optimizer, num_epochs=num_epochs, train=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(4096, 1000, batch_first=True, bidirectional=True)\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=2000, out_features=100, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=100, out_features=25, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0znm2TMmsDZ"
      },
      "source": [
        "---\n",
        "Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtMhcbXmsXk"
      },
      "source": [
        "# evalute the model by predicting over test dataset\n",
        "\n",
        "# load test dataset\n",
        "dataloader = DataLoader(test_video_dataset, batch_size=batch_size, \n",
        "                        shuffle=True, num_workers=0)\n",
        "test_loss, test_accuracy, test_time = start_train(lstm_model, dataloader, optimizer, train=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMMXAjMjv4g5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0779152c-501c-4312-8d71-9c2efbb32cbd"
      },
      "source": [
        "print('Training accuracy is %2.3f' %(train_accuracy) )\n",
        "print('Test accuracy is %2.3f' %(test_accuracy) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy is 100.000 :\n",
            "Test accuracy is 84.122 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eesNQn6FYKQz"
      },
      "source": [
        "Train and test and test accuracy of SVM:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip87hPqTYJtr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "62743d24-ce79-4a89-e42f-f4af278235d6"
      },
      "source": [
        "# SVM classification\n",
        "\n",
        "# initialization\n",
        "batch_size = 10\n",
        "trainX = []\n",
        "trainY = []\n",
        "testX = []\n",
        "testY = []\n",
        "\n",
        "# load train dataset\n",
        "dataloader = DataLoader(train_video_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "for i, data_batch in enumerate(dataloader):\n",
        "    batch_size = len(data_batch['img_list'])\n",
        "    # flatten the sequence image features\n",
        "    inputs = data_batch['img_list'].numpy().reshape([batch_size,-1])\n",
        "    labels = data_batch['labels'].numpy().reshape([batch_size])\n",
        "    trainX = trainX + inputs.tolist()\n",
        "    trainY = trainY + labels.tolist()\n",
        "\n",
        "# load test dataset\n",
        "dataloader = DataLoader(test_video_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "for i, data_batch in enumerate(dataloader):\n",
        "    batch_size = len(data_batch['img_list'])\n",
        "    # flatten the sequence image features\n",
        "    inputs = data_batch['img_list'].numpy().reshape([batch_size,-1])\n",
        "    labels = data_batch['labels'].numpy().reshape([batch_size])\n",
        "    testX = testX + inputs.tolist()\n",
        "    testY = testY + labels.tolist()\n",
        "\n",
        "# train the SVM classifier\n",
        "\n",
        "start = time.time()\n",
        "# Instantiate the SVM classifier\n",
        "model_svc = svm.LinearSVC(C=0.00103, max_iter=1000)\n",
        "# Train the SVM classifier\n",
        "train_accuracy = model_svc.fit(trainX, trainY).score(trainX, trainY) * 100\n",
        "time_taken = time.time()-start\n",
        "print ('Train Accuracy: {:.3f}, Time taken: {:.5f}'.format(train_accuracy, time_taken))\n",
        "\n",
        "# Make predictions\n",
        "\n",
        "start = time.time()\n",
        "# pred_labels = model_svc.predict(testX)\n",
        "# time_taken = time.time()-start\n",
        "\n",
        "# Find the count of correct labels\n",
        "# correct = (pred_labels == testY).sum()\n",
        "\n",
        "# Calculate the accuracy\n",
        "# accuracy = float(correct) / len(testY) * 100\n",
        "\n",
        "test_accuracy = model_svc.score(testX, testY) * 100\n",
        "time_taken = time.time()-start\n",
        "print ('Test Accuracy: {:.3f}, Time taken: {:.5f}'.format(test_accuracy, time_taken))\n",
        "\n",
        "print('Training accuracy is %2.3f :' %(train_accuracy) )\n",
        "print('Test accuracy is %2.3f :' %(test_accuracy) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Accuracy: 100.000, Time taken: 135.35001\n",
            "Test Accuracy: 85.699, Time taken: 3.32707\n",
            "Training accuracy is 100.000 :\n",
            "Test accuracy is 85.699 :\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}